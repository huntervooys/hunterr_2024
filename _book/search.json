[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hunterr_2024",
    "section": "",
    "text": "Preface\nListing¬†1: greeting\n\n\nprint(\"Good Morning! ü§ó\")\n\n\n\n\n[1] \"Good Morning! ü§ó\"",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#this-book",
    "href": "index.html#this-book",
    "title": "hunterr_2024",
    "section": "This book",
    "text": "This book\nThis document you are reading now is called a Quarto book. We will talk more about Quarto later, but for now, all you need to know about Quarto books is that they:\n\nare relatively simple to construct,\ncan contain R code snippets and their outputs,\nare pretty enough for me to be temporarily satisfied.\n\n\nFeatures of This Book\n\n\n\n\n\n\ncallouts\n\n\n\n\n\nThis book let‚Äôs me put many of my least relevant tangents in these collapsible little notes.\n\n\n\nThe features of this book, which are achieved easily through Quarto, explain its format. Code snippets appear in this book, but only if you want them to. Each code snippet is collapsible by clicking the triangle next to ‚Äúshow code for this result‚Äù. If you really hate code, you can click the ‚Äú&lt;/&gt; Code‚Äù button on the top of each page and hide (or show) all the code at once. Speaking of the ‚Äú&lt;/&gt; Code‚Äù button, if you click ‚ÄúView Source‚Äù you can see all of the code that was used to create the page.\nQuarto also lets me show you ‚Äúpaged tables.‚Äù In the following code, I generate 1000 random numbers between -10 and 10, which I label x. Then, I apply the mystery_function to each number, producing y. What does mystery_function do?\n\n\nshow code for this result\ntable_1 &lt;- tibble(\n  x = runif(n = 1000, min = -10, max = 10),\n  y = mystery_function(x)\n)\ntable_1\n\n\n\n  \n\n\n\nQuarto also let‚Äôs me show you plots, like the one below. What do you think mystery_function does now?\n\n\nshow code for this result\nqplot(x = table_1$x, y = table_1$y) +\n  labs(x = \"x\", y = \"mystery_function(x)\")\n\n\n\n\n\n\n\n\n\nIn reality, mystery_function just squares x, so:\n\\[\n\\mathtt{mystery\\_function}(x) = x^2\n\\tag{1}\\]\nAll of the chunks of code have line numbers. For any chunk, a little clipboard appears when you hover over the code listing, and you can copy it by clicking on the clipboard. (This includes the code in the ‚ÄúView Source‚Äù pane, meaning you can copy the entire page, text and all.)\nQuarto also processes citations, so I can, for example, easily direct you towards my two fathers: Turing (1936) and Foucault (1978). It will also process internal links so that I can, for example, send you back to the top of the page: Listing¬†1.\nFinally, Quarto allows me to annotate code, which is helpful to explain how it works when a high-level of technical detail is needed. For an example of those annotations and when they might be helpful, look to Figure¬†2.1.\n\n\nTools\nR is an free and open-source statistical programming language.You use R by typing commands into an R console, which looks like this:\n\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nRStudio is a graphical user interface (GUI) and interactive development environment (IDE) with which to use R. This means that RStudio contains an R console (lower left) and a variety of other tools (right), like a text editor (upper left). RStudio looks like this:\n\n\n\n\n\n\n\n\n\n\nTidyverse\n\n\n\n\n\nTidyverse is a collection of R packages that are designed to work well together and to acomplish data analysis tasks. The Tidyverse packages were developed by Hadley Wickham primarily, but also by teams of his collaborators.\nBecause Tidyverse is set of R pacakges, there is not picture to provide. Tidyverse exists as R functions that you can (and will) use in your code.\n\n\n\n\n\n\n\n\n\ngrammar of graphics\n\n\n\n\n\nThe grammar of graphics refers to another set of R packages, exemplified by ggplot2, which is included in the tidyverse. The grammar of graphics is the tool that most R programmers use to produce the ‚Äúproduction-quality graphics‚Äù for which R is known. Once you begin to produce plots with ggplot2, you are likely recognize that you have been seeing these plots for years.\n\n\n\nI believe that these tools will allow you to accomplish the vast majority of data analysis tasks, and that knowing how to use them will result in you understanding data analysis on a deeper level and being able to do it faster. That being said, these are not the only tools for data analysis. I encourage you to explore others, as well; particularly, if you intend to do a lot of work with text data, I would suggest python.\nBecause my audience does not intend to become ‚Äúprogrammers,‚Äù per se, I would also like to introduce you to the following tools, which allow R code to be integrated into readable documents instead of writing R code in separate text files with R code only (called scripts). R scripts have a place, to be clear, especially while you are learning. However, if you would like to share the analysis that you do with an R script, sharing that R script is not a good way to do that. When you send someone an R script, you are sending them a bunch of code, not an analysis\nThe current gold standard for the sort of work you are most likely to want to produce (reproducible research) is this - the document you are reading now. This is a document which includes code, the output of that code, and text explaining that code and providing context. This approach is called literate programming. The tools used to produce literate programming documents with R are:\n\nRMarkdown: a document preparation software based on R and a markup language called markdown that is mainly used to make static documents (like appendices to a journal article). RMarkdown uses a software called pandoc to turn the .rmd file into: a pdf (latex or a beamer presentation), a .html website, a word document, a power-point, and a lot of other formats you‚Äôre likely to never use.\nQuarto: a very similar, but more advanced and comprehensive software than RMarkdown. Most of the things you can do in Quarto are also possible in RMarkdown, like adding cross references (like this Equation¬†1), creating books (also like this), creating interactive data dashboards (which seems particularly trendy as of late), and creating blogs and websites. Quarto is made by the same company that makes RStudio.\nShiny: a software (written in R) that allows you to create interactive plots, which may be helpful if you are, say, trying to decide the optimal number of bins in a histogram. You could use shiny to create a histogram and a bins slider, so that you could easily see a variety of different bin sizes merely by moving a slider (instead of by writing, modifying, and rewriting code to achieve the same end).\n\nMy goal for you is to write code that is readable and to put that code inside documents that are actively fun and/or interesting to read. (That‚Äôs also, incidentally, my goal for me.)\nWe are also going to do statistics! There are two approaches to statistics that we‚Äôre going to adopt through these lessons, so I‚Äôd like to begin by elucidating the way in which these approaches are different. I am more familiar with the first approach (exploratory data analysis). Confirmatory data analysis uses many of the same tools (like hypothesis testing, which I will show you), but it uses them in a different and moer complicated way. The ultimate goal of both approaches is to predict the result of measurements.\n\nexploratory data analysis: If the purpose of a statistical model is to predict data, then a model that makes the most accurate prediction is the best model. The model creation process is iterative. Once you see the results of a model, you can use the results to modify the model itself. Generally, practitioners recognize that there are a variety of different types of models that could be used for any task. Thus, they usually construct a variety of different models and then compare them to select a final model. Practitioners will use numbers (in diagnostic and statistic tables) or visuals (like a residuals plot) when comparing models.\nconfirmatory data analysis: In the best case, at least according to the ‚ÄúOpen science‚Äù framework, the final statistical model will have been selected and preregistered before data is even collected. Statistics have to be rigorous to mean anything, a fact which the machine learning people (who do only exploratory work) ignore. They don‚Äôt check model assumptions using statistical tests like the Shapiro-Wilk normality test, and they are constantly ‚Äúp-hacking‚Äù and ‚ÄúHARKing‚Äù to forcibly extract findings from their data. Confirmatory data analysis rejects these practices, aiming instead for statistical models that are pre-specified (pre-registered), theoretically-based, and rigorous (whatever they take that to mean).\n\nI‚Äôm often somewhat flippant about the second approach, which suggest and attempts to discover Truth where I am skeptical it exists. In any case, the second approach is the only one that is taught in statistics courses. This is a mistake. Firstly, exploratory data analysis is much more commond. Secondly, it is easier to get started doing exploratory (rather than confirmatory) data analysis. Because both of these approaches adopt many of the same tools, it seems to me that starting with exploratory data analysis and trying to make that make sense is the most effective way to learn confirmatory data analysis (which will require additional effort and research that I can‚Äôt provide).\nThus, I intend to provide you with a strong understanding of data that you can directly apply to exploratory data analysis tasks. My hope is that this understanding will enable you to complete a diversity of tasks, including confirmatory data analysis, if that is of interest.\n\n\nLearning Objectives\nThere has to be some boring stuff because pedagogy. I have quite a few learning objectives for you, forming one big list, but I‚Äôll attempt to section them off so they are easier to read.\n\nR Learning Objectives\n\ndiscuss R as a language with a history: use knowledge about the history of R (and of scientific computing more generally) to describe what R is, what people ‚Äúsay‚Äù in this language, and why this language has the properties and characteristics that it does.\nR competence: read R expressions written by others (allowing the language to serve a communicative purpose), and write R expressions that are readable and align with best practices within the open source R community.\nR‚Äôs friends: Describe R‚Äôs relationship to RStudio, RMarkdown, Shiny, Quarto, and Tidyverse; and, describe what each of these tools is and why someone would use them.\n\nrun R code in several different ways: via the console, a script, and Quarto or RMarkdown documents.\n\ndescribe R‚Äôs data types and the use of each: strings, numerics (floating point ‚Äúdoubles‚Äù, integers, and complex numbers), logicals, datetimes, and factors\ndescribe R‚Äôs data structures and the use of each: including, vectors (1-dimensional arrays), matrices (2-dimensional arrays), arrays (more than 2-dimensional arrays), lists (key-value pairs), data frames, and tibbles\naccess R documentation, and read it effectively enough to solve a problem\n\n\n\nComputation Learning Objectives\n\ngenerate synthetic data: use simulation of simple events (like the rolling of dice or flipping of a coin) to gain visual intuition for the central limit theorem and the law of large numbers\nuse the Monte Carlo simulation framework to evaluate statistical tests (e.g., by determining what happens when assumptions are violated)\nprocess string data: convert strings to all upper or lower case, add prefixes or suffixes, splitting strings apart\nprocess numeric data: scale and center numeric data and write functions to accomplish non-standard transformations\nprocess language data: apply the principles of natural language processing to pre-process text data (by tokenizing and stemming text and describing both of those processes and why they are used)\n\n\n\nStatistics Learning Objectives\n\nnull hypothesis significance testing: use R to perform null-hypothesis significance tests, such as the one-sample, two-sample, and repeated measures t-test\nregression: use R to perform linear and logistic regressions, including regressions with polynomial terms\nmachine learning: use R to perform a more complicated machine learning task, likely by constructing a decision tree and a random forest classification model\ndimensionality reduction: perform principal component analysis and construct a latent semantic space, and explain why these two seemingly distinct methods are connected by singular value decomposition\n\n\n\nData Science Learning Objectives\n\nrectangular data: use the tidy data framework to read, write, and pre-process rectangular data in a consistent, efficient, and minimally complex manner.\n\nimport data from a variety of sources including: comma-separated values (.csv) files, excel spreadshees (.xlsx files), google sheets spreadsheets\n‚Äòtidy‚Äô data into the following format: one observation per row, one variable per column, one value per cell\nuse available tools that enable you to store data in a very consistent format with very little effort\n\nlanguage data: use R to pre-process, analyze and visualize text data\nvisualization: use R and the grammar of graphics (represented by ggplot2 and related packages) to visualize data and to share data visualizations with others\npublication: use RMarkdown or Quarto to conduct a linear or logistic regression, and to report and interpret the results of those tests\n\n\n\n\n\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_lore.html",
    "href": "01_lore.html",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1 Early 19th Century: the birth of programming\nThe idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; then, the machine part of the computer tries to read find the and read the program at the specified location, and the (hopefully) program runs.\nFolks in the 19th century did not have access to digital text files, and so they could not write their programs on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this very first section?",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-birth-of-programming",
    "href": "01_lore.html#early-19th-century-the-birth-of-programming",
    "title": "1¬† The Lore",
    "section": "",
    "text": "1.1.1 Jacquard‚Äôs Loom: the punched card\nOne of the first programmable machines was Jacquard‚Äôs loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard‚Äôs loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard‚Äôs punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830‚Äôs he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division; it was designed to take input via punched card, just like Jacquard‚Äôs loom.\n\n\n\n\n\n\nLady Lovelace\n\n\n\n\n\nDuring the 1830‚Äôs and 1840‚Äôs, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, daughter of the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific symphonies (Tibees 2020). Isn‚Äôt that beautiful!",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#middle-and-late-19th-century",
    "href": "01_lore.html#middle-and-late-19th-century",
    "title": "1¬† The Lore",
    "section": "1.2 Middle and Late 19th century",
    "text": "1.2 Middle and Late 19th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution left white men horrified at the peculiar institution, which was also losing economic utility (see Capitalism and Slavery by Eric Williams).\nAlso within that 40 year period was:\n\nabolition (as a matter of law, anyways): British Empire (1834), French Empire (1848), Russian Empire (1861), Dutch Empire (and Dutch East India Company; 1863-73), American Empire (1865), Portuguese Empire (1869)\nSamuel Colt‚Äôs invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman‚Äôs rights convention in the U.S. (Seneca Falls Convention, 1848)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling‚Äôs invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction and openning of the Suez Canal (1860‚Äôs)\nMendel‚Äôs publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel‚Äôs invention of dynamite (1867)\nMarx‚Äô publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev‚Äôs publication of the first periodic table (1869)\n\nIn this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.\n\n\n1.2.2 Late 19th century\nWith the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.\nWilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median (Bakker and Gravemeijer 2006). Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.\nAlso in the late 19th century was what Foucault called the implantation of perversions (Foucault 1978) - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.\nFirstly, there was the medical specification of the homosexual (Townsend 2011). This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female (Townsend 2011). The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.\nAlso within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.\nI‚Äôll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of ‚Äúgood moral character‚Äù were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.\nUntil 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the ‚Äúillegal alien‚Äù was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.\nAll this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.\nI consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I‚Äôm going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.\nThe late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.\nAnother monopoly was also forming. Using Jacquard‚Äôs punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#early-19th-century-the-advent-of-computing",
    "href": "01_lore.html#early-19th-century-the-advent-of-computing",
    "title": "1¬† The Lore",
    "section": "1.3 Early 19th century: the advent of computing",
    "text": "1.3 Early 19th century: the advent of computing\nNear the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:\n\nto be consistent: it should not be possible to derive that a statement is both true and false\nto be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)\nto be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.\n\n(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that \\(1+1=2\\). I‚Äôm mostly attributing these three demands to Hilbert for sanity‚Äôs sake because I cannot stand to write out the sordid details. These three ‚Äúproperties‚Äù as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of 23 questions, which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)\nMathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein‚Äôs theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.\nNeils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920‚Äôs quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The ‚Äúdemands of power‚Äù did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.\nDuring my quantum mechanical tangent, G√∂del has proven that achieving the second property of Hilbert‚Äôs idealistic system is unlikely. In fact, G√∂del establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.\nTo answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements (Turing 1936). There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).\nThus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term Turing-complete which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in (Turing 1936). The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard‚Äôs loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from (Turing 1936).\nThe first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine ENIAC. Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine (Shustek 2016). The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).\nInitially, the wires on ENIAC had to be moved for each new problem (Shustek 2016). The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.\nThis machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.\nThis is the idea behind modern programming languages. Instructions for the computer, written in the computer‚Äôs language (ENIAC‚Äôs language was wires, the one we‚Äôll soon focus on is R) are stored within the machine. ‚ÄúProgramming‚Äù the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC ‚Äúprogram‚Äù ran under this new computer architecture was a Monte Carlo simulation of neutron decay during nuclear fission (Shustek 2016).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "01_lore.html#att-bell-labs-and-s",
    "href": "01_lore.html#att-bell-labs-and-s",
    "title": "1¬† The Lore",
    "section": "1.4 AT&T, Bell Labs, and S",
    "text": "1.4 AT&T, Bell Labs, and S\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T‚Äôs service, nor did it prevent future anti-trust lawsuits. Throughout the 60‚Äôs and early 70‚Äôs, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nBell Labs was probably the most important laboratory within AT&T. During the early 70‚Äôs, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).\n(This isn‚Äôt super relevant, but I think it‚Äôs fun. Before 1968, computational statisticians had been using a algorithm called RANDU, which was a FORTRAN function that generated random numbers. In 1968, a mathematician proved that the allegedly random numbers actually all had to lie on a series of parallel hyper-planes, and we thus not actually random. wtf is a series of parallel hyper-planes? See below)\n\n\n\n\n\n\n\n\n\nFORTRAN, the name, stands for ‚Äúformula translation,‚Äù and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on its website.\nIn any case, in the 1970‚Äôs, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs (Becker 1994).\n\n1.4.1 S\nS is a statistical computing language that was developed first at Bell Laboratories in the mid 1970s. At the time, statistics was undergoing a change. Previously, statistics had been developing as a set of methods - essentially algorithms that prescriptively described how to complete a statistical analysis from beginning to end. In the early 70‚Äôs, John Tukey was working at Bell Labs and at Princeton, and he was making a lot of noise about the problems with statistics. He popularized a different approach to statistics, establishing something of a binary between data analysis and statistics, just as I did between machine learning and statistics (in Section 1.2; Tukey (1972)).\nThe statistics research department was beginning to demand a tool that aligned with Tukey‚Äôs approach. FORTRAN, developed more than a decade before that demand was created at and by Bell Labs, did not measure up to the task. Instead, they decided to develop a new language, which they named S. Initially, there was a large focus on being able to import FORTRAN functions into S, so that there could be a smooth transition from FORTRAN to S within Bell Labs.\nS was built from the ground up to include graphics capabilities, and a structure that enabled and encouraged exploratory data analysis. The basic data structure in S is a vector of like-elements, which were used to make matrices and time-series; S also included lists (key-value maps) and the $ operator, which could be used to retrieve specific components of larger data structures (Becker 1994). It also included all of the arithmetic operators that you need in a desk calculator, making it useful for that purpose, as well.\nIn 1980, S was distributed outside of Bell Labs for the first time. Initially, it was distributed for a nominal fee and for educational use only, but by 1981 it was widely available (Becker 1994). After it began to be distributed, the developers added explicit looping (i.e., for loops), as well as the apply function, which could be used to loop over a vector while applying a function (Becker 1994). The developers also introduced the ‚Äúcategory‚Äù, which is now called the factor in R. Categories are vectors of data. They merge numerical and string data types - each entry in the vector is assigned a category label (so that you can read it), as well as a underlying integer (so that you can do math with categories).\nAlthough S was developed initially by statisticians, it clearly had utility as a data manipulation, graphics, and exploratory data analysis tool. In 1988, the developers released the ‚ÄúNew S,‚Äù renaming the software after some significant changes. The most significant feature of New S was the inclusion of first class functions, which are functions that you can assign to a name and and then refer to by that name. Functions are first class in that they are S objects, just like any vector or matrix. For the first time, S had depreciated functions, which R also has. Depreciated functions are functions for which there is a better alternative. They are generally still included in R and S distributions (so old code that uses depreciated functions can still run), but it‚Äôs best to avoid using them (and to use the better alternatives instead). By 1988, many of the FORTRAN functions from the initial development of S were rewritten in C, which is a general purpose programming languages on which New S is built (Becker 1994).\nIn 1991, the S development team expanded, and there was a focus on adding statistical software to the S language. Although S was developed by statisticians who intended to use it for statistics, the statistics are not inherent in S: ‚ÄúS is a computational language and environment for data analysis and graphics‚Äù (Becker 1994). As such, the developers added the formula class, which could be used to specify statistical models. The formula is marked by the ~ operator, with the dependent variable on the left and the independent variable(s) on the right (e.g., y ~ x + w + x*w).\nAlso in the 1991 release was the data.frame. Matrices are like vectors: they can only have one type of data. If you have a matrix that has even one number in it, then the entire matrix must be numeric, even if you want to use it to represent string data (like names and job titles) or categorical data (Becker 1994). So, a matrix is a combination of multiple vectors, all of the same type. A data.frame, by contrast, is a combination of vectors of any type. You can have a string vector (column) in the data frame representing job title, as well as a numeric vector representing income. As with matrices, you can use the $ operator to pick a vector out of the data frame (e.g., data$income picks out the income vector in the data frame called data).\nIt‚Äôs not really possible for a programming language to die. As we have seen with FORTRAN and S, new programming languages often use code from their older counterparts, especially at the beginning. Even though I can no longer find S on the internet and run it on my computer, a very large number of S functions continue to exist in R.\nI am able to find relatively scant documentation about this final period in the history of S, so the rest of this section is at least somewhat speculative (except claims that are cited, of course).\nWhat is the need for R if S exists? Well, well, well. Let‚Äôs talk about corporate fuckery, which both killed S and prevented it from dying. I have been making a much bigger deal over anti-trust law than the vast majority of those who introduce R to their students. To this point, as far as AT&T and Bell Labs are concerned, I have presented a world in which they are legally prohibited from selling computers (and presumably, computer software) as a result of the Consent Decree from 1956.\nUp to this point, no one was making money off of S. Although Bell was initially charging folks a nominal fee to use the software (Becker 1994), this practice ended quickly, meaning that the software was being distributed for free. As a result of the anti-trust, Bell Labs was not going to monetize this technology. Instead, one of their former employees had to do it.\nIn the 70s and 80s, the graphical user interface (GUI) was being born. This emerging technology came with a new generation of capitalists who had not been subject to extensive anti-trust, in which former trade union president Ronald Reagan did not believe - the capitalists who bring us Microsoft and Apple, who own outright the operating systems of about 85% of the worlds‚Äô computers (and many phones and other devices, as well).\nS wasn‚Äôt fated to become Windows; it was fated to become S-PLUS. S-PLUS is/was a statistical computing software with a graphical user interface. It was developed by a company owned by a former Bell Labs employee and University of Washington professor, R Douglas Martin. His work is primarily in econometrics, and he has extensively published about investment risks. Because of this, and because S-PLUS was and is mostly used by economists, a cynic might call it an application to be used for those who are unwilling or unable to learn how to code (similar in character to Microsoft‚Äôs SPSS). S-PLUS started circulating (for a fee) in about the year 1987, and it did include features that S did not (like generalized linear models).\nLet me just quickly recap, so I can make sure everyone is oriented in time - I‚Äôm discussing a lot of events that overlap and are not all well documented. In 1980, S was released to the public; in 1988, S had a significant update, becoming ‚ÄúNew S‚Äù; around 1987, a former employee of Bell Labs developed S-PLUS; in 1991, S had an update that focused on statistics.\nIn 1991, two statisticians quietly began work on the project (R) that would more-or-less kill S and S-PLUS.\nIn 1993, S and S-PLUS were reunited when Bell Labs sold S to the company that had developed S-PLUS. That company, in turn, immediately merged with a company called MathSoft. S-PLUS was only available on windows, and its relationship with Microsoft strengthened when features were added to connect S-PLUS to Excel and to SPSS.\nPart of the company (MathSoft) was sold, it got renamed (to Insightful), the exclusive license to distribute S turned into AT&T (i.e., Lucent, one of the companies that remained after AT&T) selling S so that it became the property of Insightful. Then Insightful got bought by a company called TIBCO, and then‚Ä¶\nI think you get the general idea. S and S-PLUS got bought, and sold, and licensed, and merged, and acquired to the point that it no longer really exists in any meaningful, public way. But by the 2000s, that didn‚Äôt matter.\n\n\n\n\n\n\nBakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn Historical Phenomenology of Mean and Median.‚Äù Educational Studies in Mathematics 62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of S.‚Äù In Computational Statistics, edited by Peter Dirschedl and R√ºdiger Ostermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase Study 1: AT&T Divestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC: An Example of Why Computer History Is Hard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of ‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone Projects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and Mathematics.‚Äù Quarterly of Applied Mathematics 30 (1): 51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to the Entscheidungsproblem.‚Äù Journal of Math 58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html",
    "href": "02_r_and_friends.html",
    "title": "2¬† R and Friends",
    "section": "",
    "text": "2.1 R and Friends\nIn 1991, two professors in New Zealand began to develop R, a process which they documented in (Ihaka and Gentleman 1996). R is very similar to S; so similar, in fact, that it is frequently called a dialect of S. What is the difference between S and R? The creators of R describe it as having the syntax of S (meaning that most examples, including the following example, can be run in S or R) but the semantics of Scope (which is a programming language from the Lisp family).\nProbably the key difference between the two languages is the lexical scoping. Whenever you use R (or most other programming languages), you have to have something called a frame. Frames include things like functions and named variables. Each function creates its own frame. The frame for the function f in the example below (from Ihaka and Gentleman (1996)) contains the named variable y and the named function g. The named function g, as a function, creates its own frame (in which to store variables and functions). There is also something called a global frame which, in the following example, includes an assignment of the value 123 to the name y and the assignment of some function to the name f.\nAs you can see, in R, running the function f with 10 as an argument results in the function returning 100 (10 times 10). In S, this very same code would have resulted in function f returning the value 123. In S, when we define the function g, S uses the global frame as the basis for the function, including the assignment of the value 123 to y. R, by contrast, creates g with a locally-scoped frame, meaning that the frame for g includes the assignment of the value x * x to the variable y (assignments which are inherited from the parent frame). Thus, in S, the function g is evaluated as print(123), but the R function is evaluated as print(x * x) (the function f is responsible for substituting x to make print(10 * 10).\nUnlike Scheme, but like S, R uses lazy evaluation. In essence, this means that R does not run your code unless it absolutely has to. I‚Äôll use the example of Figure¬†2.1 to explain what this means. Lines 2 through 6 contain the declaration of function f (even though line 4 also contains the declaration of function g. When you run line 2, all of the lines down to line 6 (where the closing bracket, } is located) get stored in R‚Äôs memory next to the name f. However, R will not run the function f until you actually go to use it (i.e., until you make the function call in line 7). This is why we call R lazy, but what‚Äôs the big deal?\nIf you make a syntax error in your declaration of function f, R is going to have to tell you that you made a syntax error at some point. In a language that is not lazy, the language evaluates function f when you store it. Thus, a non-lazy language will send you a syntax error after you run the function declaration (i.e., after you run line 2, which also causes lines 3-6 to run). If Figure¬†2.1 were written in a non-lazy language, the syntax error would occur where the 1 annotation is. However, in R, the function is merely stored when you run lines 2-6. Function f does not actually run until you call it in line 7 (marked with a 3 in Figure¬†2.1). Laziness is a feature that R inherited from S, which is also lazy.\nThis is getting a bit technical. The two men who developed R are Ross Ihaka and Robert Gentleman. On a family tree posted on Ihaka‚Äôs personal website, he lists himself as the academic grandchild of John Tukey, then statistician at Bell Labs that popularized exploratory data analysis (the framework that created the need for S, which was also, if you‚Äôll recall developed at Bell Labs).\nIhaka is a now retired statistician from the University of Auckland. Gentleman is a bioinformatician who currently works at Harvard and 23andMe. Allegedly, Ihaka and Gentleman developped R for use in teaching statistics. That was part of both of their jobs as professors, after all. However, this doesn‚Äôt seem very plausible (why would the professors write their own statistical programming languages rather than using a well-documented one which would seem to be better for pedagogy), nor have I seen any specific evidence for it. That being said, in the years 1993-94, R was stuck at the University of Auckland, being used by them, probably their peers, and less probably their students, but the software was not yet being distributed, as S or S-PLUS was.\nIn 1995, one of their colleagues convinced them to licence use of the software as free software under a GNU general public license.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R and Friends</span>"
    ]
  },
  {
    "objectID": "02_r_and_friends.html#r-and-friends",
    "href": "02_r_and_friends.html#r-and-friends",
    "title": "2¬† R and Friends",
    "section": "",
    "text": "show code for this result\ny &lt;- 123 # assign 123 to the name y\n1f &lt;- function(x) {\n  y &lt;- x * x # assign x times x to the name y\n2  g &lt;- function() print(y) # create new function and new scope\n  g() # return the output of function g\n}\n3f(x=10)\n\n\n\n1\n\nthe the beginning of the declaration of function f (between lines 2 and 6)\n\n2\n\nthe declaration of function g\n\n3\n\na function call for function f (with the argument x set to equal 10)\n\n\n\n\n\n\n\n\n[1] 100\n\n\n\n\nFigure¬†2.1: an example from Ihaka and Gentleman (1996)\n\n\n\n\n\n\n\n\n\n\n2.1.1 Free Software\nI just bolded the term free software; why? As it turns out, the term free software has a specific definition that extends far beyond the idea of ‚Äúsoftware that you don‚Äôt have to pay for.‚Äù So what is free software? Free software is characterized by the four freedoms (Foundation n.d.):\n\n\n\nFree Software Foundation‚Äôs Four Essential Freedoms\n\n\nThe idea of free software, and it‚Äôs formation in the four freedoms seen above, came to be popular in the mid-80‚Äôs after the Reagan government had made clear it‚Äôs stance (and the republican, and soon the democratic, party‚Äôs stance) on anti-trust enforcement. In the wake of the disruption to the computing (IBM) and telephone (AT&T) industries, the Reaganites declared that we were entering into an era of free, competitive trade while setting up a regulatory framework that would allow tech companies to consolidate power and market share ad infinitum, resulting in the current big 4(-ish): Apple, Alphabet (Google), Amazon, and Meta (and Microsoft, Nvidia, and potentially Tesla and like Netflix, depending on who you ask).\nIt is a good thing for us, then, that none of these companies own R, which the developers have promised will remain free software indefinitely.\n\n\n2.1.2 Open Sourcing and Crowd Sourcing\nThese days, it feels like only a real purist will call R free software. The more en vogue term is ‚Äúopen source.‚Äù The Free Software Foundation would like you to treat the terms as separate however (see this article).\nIn reality, the labels ‚Äúopen source‚Äù and ‚Äúfree software‚Äù are mostly synonymous in that they refer to many of the same software. As Freedoms 1 and 3 make clear, software must be open source before it can be free. The free software folks‚Äô biggest problem with ‚Äúopen source‚Äù is one of semantics, really. They claim that the ‚Äúopen source‚Äù movement argues too much about how free software is good for business and software development (i.e., because curious users can look for and find bugs). The free software people are not interested in these practical matters, focusing instead of the moral question of what sort of software is right and wrong. They correspondingly argue their case in the form of moral imperatives (the four freedoms).\nI am less interested in these theoretical questions, and more interested in explaining to you what the implication of free or open software is bound to be (at least in the case of R): crowd-sourced development.\nR itself provides you with basic statistical functionality. However, the vast majority of what is commonly called ‚ÄúR‚Äù is not actually part of the base distribution of R. Instead, most of the functionality is packaged within ‚Äúpackages,‚Äù which are you load into R with the library() function, as shown below:\n\n\nshow code for this result\nlibrary(tidyverse)\n\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\n\nHadley Wickham\n\n\n\n\n\n\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free Software? - GNU Project - Free Software Foundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A Language for Data Analysis and Graphics.‚Äù Journal of Computational and Graphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>R and Friends</span>"
    ]
  },
  {
    "objectID": "03_r_101.html",
    "href": "03_r_101.html",
    "title": "3¬† R 101",
    "section": "",
    "text": "3.1 Introduction\nThis tutorial is adapted from a fabulous youtube video by Very Normal. I recommend watching this video before starting this tutorial.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-types",
    "href": "03_r_101.html#data-types",
    "title": "3¬† R 101",
    "section": "3.2 data types",
    "text": "3.2 data types\n\n3.2.1 Numeric Data\n\n\nshow code for this result\na &lt;- 12 # numeric\nb &lt;- 5L # integer\nc &lt;- 12.5 # double (floating point)\n\nprint(a + b) # numeric \n\n\n[1] 17\n\n\nshow code for this result\nprint((a + b) / b)  # double (floating point)\n\n\n[1] 3.4\n\n\n\n\n3.2.2 Complex Numbers\nRealistically, you‚Äôre never going to use complex numbers, so you can safely skip this exercise. However, I think complex exponentiation is pretty, so you‚Äôd really be missing out.\n\n\nshow code for this result\n# change real and imaginary parts to see how the plot changes\nz &lt;- (0.8 + 1i)\n\n# take z to the power of 1, 2, ..., whatever the 'to' argument is\ncomplex_numbers &lt;- z ** seq(from = 1, \n                      to = 12, # change to plot more data \n                      by = 0.1) # change to plot more data\n\n# plot the complex numbers\nplot(complex_numbers, type = \"o\") # possible types: \"p\", \"l\", \"b\", \"c\", \"o\", \"h\", \"s\", \"S\", \"n\"\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Characters\n\n\nshow code for this result\na &lt;- \"hello\"\nb &lt;- 'world'\n\nprint(paste(a, b))  # paste is a function that concatenates strings\n\n\n[1] \"hello world\"\n\n\nshow code for this result\nprint(paste0(a, b))\n\n\n[1] \"helloworld\"\n\n\n\n\n3.2.4 Logical Data\n\n\nshow code for this result\nTRUE & FALSE\n\n\n[1] FALSE\n\n\nshow code for this result\nTRUE | FALSE\n\n\n[1] TRUE\n\n\n\n\n3.2.5 factors\nfactors combine numbers and strings. They are useful for categorical data.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#data-structures",
    "href": "03_r_101.html#data-structures",
    "title": "3¬† R 101",
    "section": "3.3 Data Structures",
    "text": "3.3 Data Structures\n\nvectors: stores ordered data of the same type\n\nhave indexes that start at 1\n\nmatrix: stores data in 2 dimensions\narrays: higher dimensional matrices\nlists: key value pairs\n\nif you don‚Äôt put names in the list, R will index them with numbers\n\ndataframes: each row is an observation, each column is a characteristic\ntibbles: data frames with extra functionality",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#iteration",
    "href": "03_r_101.html#iteration",
    "title": "3¬† R 101",
    "section": "3.4 Iteration",
    "text": "3.4 Iteration\n\nfor loop: will run code a certain number of times\nwhile loop: will run code until a certain condition is no longer met\n\nuseful in optimization tasks (like sample size calculations)",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#control-flow",
    "href": "03_r_101.html#control-flow",
    "title": "3¬† R 101",
    "section": "3.5 Control Flow",
    "text": "3.5 Control Flow\nif (condition) {\n  code to run if condition is met\n}\nyou can add else statements, and even chain else statements, but it is easy to get confused",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#functions",
    "href": "03_r_101.html#functions",
    "title": "3¬† R 101",
    "section": "3.6 functions",
    "text": "3.6 functions\nfunction_name = function(inputs) {\n\n  a bunch of code you would like to reuse\n  the last line of code is the output of the function\n\n}",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#libraries",
    "href": "03_r_101.html#libraries",
    "title": "3¬† R 101",
    "section": "3.7 Libraries",
    "text": "3.7 Libraries\n\nhow to load and detach libraries\ndevtools - downloading from github\nCRAN",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#rstudio",
    "href": "03_r_101.html#rstudio",
    "title": "3¬† R 101",
    "section": "3.8 RStudio",
    "text": "3.8 RStudio",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "03_r_101.html#tidyverse",
    "href": "03_r_101.html#tidyverse",
    "title": "3¬† R 101",
    "section": "3.9 Tidyverse",
    "text": "3.9 Tidyverse\n\n3.9.1 readr\n\nread_csv and related funcions for reading data\nreadxl::read_xlsx for excel files\n\n\n\n3.9.2 tibble\n\nhow to make a tibble\nalmost all of the functions in the tidyverse input and output tibbles; we can pipe data\n\n\n\n3.9.3 dplyr\n\nmanipulation of data, especially useful for cleaning\nselect: select or remove\nfilter\nmutate: create new columns\n\n\n\n3.9.4 stringr\n\n\n3.9.5 lubridate\n\n\n3.9.6 forcats\n\n\n3.9.7 purr\n\nlist columns are super useful - lists can have different data types\nmap functions - output is a list column\n\n\n\n3.9.8 tidyr\n\npivot wider\npivot longer\n\n\n\n3.9.9 ggplot2\n\nmappings\ngeom - like geom_line or geom_point\nyou can add as many layers as you want\nthemes",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>R 101</span>"
    ]
  },
  {
    "objectID": "04_visualization.html",
    "href": "04_visualization.html",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1 Plotting Basics\nThe first step to making any plot is calling the ggplot function. If you look at the help page for the ggplot function (by typing ?ggplot into an R console) , you will find that it takes 2 arguments: data and mapping. Ggplot expects that the data argument is going to be a data frame with tidy data in.\nshow code for this result\nggplot(data = time_use)\nAs you can see, ggplot creates an empty plot. Next, we will add a mapping argument. The mapping will tell ggplot which aesthetics (like the x-axis, y-axis, color, shape, etc.) will be represented by which variables in the data. In this case, we want the time_spent variable on the x-axis and the women_to_men variable on the y-axis, as they appear in the reference plot. When we call the ggplot while providing a data and a mapping argument, R will create a blank plot with axes.\nshow code for this result\nggplot(data = time_use, mapping = aes(x = time_spent, y = women_to_men))\nAs with any function in R, we do not need to write all of the arguments on a single line. We can separate the arguments onto different lines, as long as we make sure each line ends with a comma or the end parenthesis (i.e., the end of the function call). You may find that the following two code snippets are more readable than the one above, even though all three would produce the same output.\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men))\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(\n    x = time_spent,\n    y = women_to_men\n  )\n)\nTo R, it does not matter whether you type this function call out in 1 line or in 7; it is the same arguments being passed to the same function and thus produces the same result.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plotting-basics",
    "href": "04_visualization.html#plotting-basics",
    "title": "4¬† Visualization",
    "section": "",
    "text": "4.1.1 Geoms\nOur plot is missing something: can you spot it? There is no data on our plot! We represent data in a ggplot by using a geom function. Almost all of these functions start with geom_ (like geom_bar or geom_smooth) or stat_ (like stat_count). You can see a fuller list of the geom functions available on the ggplot2 cheat sheet.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nWe add a geom by literally adding it (with the + operator) to the ggplot function call. The + always has to go at the end of the line. You can separate the functions by as many empty or commented lines as you‚Äôd like. You can also do this within function calls. So, this code will run:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  \n  \n  # this is a comment\n  \n  geom_point()\n\n\n\n\n\n\n\n\n\nas will this code:\n\n\nshow code for this result\nggplot(data = time_use,\n       \n       # this is a comment\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\nand if you wanted to be really verbose, you could even do something like this:\n\n\nshow code for this result\n# create ggplot\nggplot(\n  \n  # add data to plot\n  data = time_use,\n  \n  # create mapping\n  mapping = aes(\n    # map x-axis\n    x = time_spent,\n    # map y-axis\n    y = women_to_men\n  )\n) +\n  \n  # add scatterplot\n  geom_point()\n\n\n\n\n\n\n\n\n\nWhat is the difference between a mapping and a style? A mapping connects one aesthetic to a variable, but a style just sets the aesthetic. For example, styling our scatter plot might mean turning all the points blue, whereas a mapping would match each activity to a color based on a scale. This is a graph that uses color as a style:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(color = \"blue\")\n\n\n\n\n\n\n\n\n\nand this is a plot that uses color as a map:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity))\n\n\n\n\n\n\n\n\n\nYou can put mappings in the ggplot function, or in any geom function. In the above code, the mapping for x and y is in the ggplot function, and the mapping for color is in the geom_point function.\nEvery geom will inherit the mapping from the ggplot function. If we added another geom to the plot, we could see this. In the plot below, I added the geom_smooth, and - as you can see - it inherits the x and the y aesthetic from the ggplot function, but it does not inherit the color argument from the geom_point function.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = activity)) +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIf you wanted R to make differently colored smooth lines in this plot, you could add a color aesthetic to the geom_smooth function, or you could move the color aesthetic from the geom_point function to the ggplot function, thereby allowing the geom_smooth function to inherit a color aesthetic.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, \n                     y = women_to_men,\n                     color = activity)) +\n  geom_point() +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLet‚Äôs do a little investigating with just the geom_smooth function.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe used the color aesthetic to make separate lines above, but we didn‚Äôt need to. We can also use the group or linetype arguments to create separate lines\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(linetype = activity))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWe can change the data supplied to geom_smooth to draw only one line:\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_point(mapping = aes(color = continent)) +\n  geom_smooth(data = filter(time_use, activity == \"Unpaid work\"),\n    mapping = aes(group = activity))\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou should use the help page (?geom_smooth) to read more about the aesthetics and arguments the function can take. I will only highlight two more: method, and se. I frequently find myself favoring a linear regression line (rather than the default which uses the LOESS smoothing function (a type of local, polynomial regression). You can get a straight line by changing the method argument to ‚Äúlm‚Äù. The se argument can be set to T or TRUE or F or FALSE, and it controls whether the plot includes an gray error area.\n\n\nshow code for this result\nggplot(data = time_use,\n       mapping = aes(x = time_spent, y = women_to_men)) +\n  geom_smooth(mapping = aes(group = activity), \n              method = \"lm\",\n              se = FALSE)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nOn a completely separate note, you use a very similar process of setting the data and mapping arguments to make a bar chart. Ggplot makes a distinction between a bar chart and a column chart (even though they can appear to be identical). A bar chart has a single, discrete aesthetic mapping (like one that maps x to continent).\n\n\nshow code for this result\nggplot(data = distinct(time_use, country, continent),\n       mapping = aes(x = continent)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\nA column chart takes 2 mappings: a discrete x aesthetic (like activity) and a continuous y aesthetic (like time_spent).\n\n\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                color = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nBar charts are a great example of the difference between the color and the fill aesthetics. Points and lines have only colors, but bars and columns (and other geoms, like density plots) have fills, as well. Let‚Äôs fix the last plot.\n\n\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col()\n\n\n\n\n\n\n\n\n\nThis plot does not provide a helpful comparison of the time use in OECD countries across various continents because all of the columns have a different height. In reality, we would like all of the columns to be the same height so we can compare proportions. We can do this using the position argument\n\n\nshow code for this result\nggplot(\n  data = time_use,\n  mapping = aes(x = continent, \n                y = time_spent,\n                fill = activity)\n) +\n  geom_col(position = \"fill\")\n\n\n\n\n\n\n\n\n\nThis is a better plot to view the differences in OECD time use across continents. However, there is (again) very limited data for several continents which are either missed (i.e., most of Africa, Oceania, South America, and Asia) or have only very few countries (i.e., North America)\n\n\n4.1.2 Faceting\nUsing just the ggplot function and a few geom functions, we can get damn near the reference plots we started with.\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFaceting is tricky to explain in words, but it‚Äôs very easy to understand if you can see it.\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are two faceting functions: facet_wrap and facet_grid. Both have a scales argument, which is set as ‚Äúfixed‚Äù by default. You can change it to:\n\n‚Äúfree_x‚Äù to allow the x axes to have different limits in the different plots\n‚Äúfree_y‚Äù to do the same for the y axes\n‚Äúfree‚Äù to have both axes take different limits for each separate plot\n\nIn this case, we‚Äôll just set the scales argument to \"free\" so that all of the data aren‚Äôt packed so tightly.\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nfacet_grid is helpful if you would like to facet on two (discrete!) variables at once (like activity and continent in the plot below).\n\n\nshow code for this result\n# initialize plot and axes\nggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 0.5,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_grid(continent ~ activity, \n             scales = \"free\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWith so few data, such extensive faceting is neither informative nor helpful, so we‚Äôll stick with just one facet: activity.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "04_visualization.html#plot-styling",
    "href": "04_visualization.html#plot-styling",
    "title": "4¬† Visualization",
    "section": "4.2 Plot Styling",
    "text": "4.2 Plot Styling\nOur current plot is beginning to look very similar to the reference plot we were attempting to recreate. We have written 23 lines so far. To save space, I am going to save the plot that we currently have under the name p. When we call p, R will make the plot. We can also add things to p, just as we added them to the ggplot function.\n\n\nshow code for this result\n# save ggplot object under the name \"p\"\np &lt;- ggplot(data = time_use,\n       mapping = aes(x = time_spent,\n                     y = women_to_men)) +\n  \n  # add scatter plot\n  geom_point(mapping = aes(color = continent,\n                           shape = continent)) +\n  \n  # add black regression line\n  geom_smooth(color = \"black\",\n              size = 2,\n              method = \"lm\",\n              se = FALSE) +\n  \n  # add dashed parity line (y = 1)\n  geom_hline(yintercept = 1, \n             linetype = \"dashed\") +\n  \n  # add a facet - activity\n  facet_wrap(~activity, \n             nrow = 2,\n             scales = \"free\")\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n4.2.1 Labels\nBy default, ggplot does not create a title or subtitle for the plot, and it uses the names of the variables for the axes and scales (i.e., color scale in our plot). The labs function allows you to update various text elements in the plot.\n\n\nshow code for this result\np &lt;- p + labs(title = \"Gender Parity in Time Spent in OECD Nations\",\n         subtitle = \"ATTN: all plots on different axes\",\n         x = \"time spent (minutes per day)\",\n         y = \"ratio of women's time spent to men's\")\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nAdding labels to the outside of your plot is as simple as that. You could also use labels as a geom using the geom_text function. It takes an additional aesthetic, label that we have not yet seen.\n\n\nshow code for this result\nggplot(data = filter(time_use, activity == \"Unpaid work\"),\n       mapping = aes(x = time_spent,\n                     y = women_to_men,\n                     color = continent)) +\n  geom_text(mapping = aes(label = country))\n\n\n\n\n\n\n\n\n\n\n\n4.2.2 Scales and Coordinates\nOur plot is different from the reference plot in which colors it uses. The reference plot uses black, orange, blue, green, and yellow; but, ours uses red, beige, green, blue, and purple. What gives? The reference plot uses a different color scale than does our plot. The color scale is the part of the plot that matches each continent to a unique color. The reference plot uses a color scale from the ggthemes package that is colorblind friendly.\n\n\nshow code for this result\np &lt;- p + ggthemes::scale_color_colorblind()\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nscales also control the x and y axes, specifically continuous scales. We can use a scale function to set the limits or breaks for the axes. In our case, this allows us to demonstrate how much larger the gender disparity appears to be for unpaid work rather than for any of the other (measured) uses of time.\n\n\nshow code for this result\np + \n  scale_y_continuous(limits = c(0, 7), breaks = c(1, 3.5, 7))\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nwe can use scale functions to add transformations, as well. There are functions like scale_x_sqrt and scale_x_log10 that put the data on square root or log axes. Our time use data is not a great example of the utility of these types of axes. Instead, look at this data about coffee production:\n\n\nshow code for this result\ncoffee_plot &lt;- ggplot(data = coffee,\n       mapping = aes(x = pounds,\n                     y = pop_2019)) +\n  geom_text(mapping = aes(label = country)) +\n  labs(x = \"pounds of coffee produced in 2019\",\n       y = \"population in 2019\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\ncoffee_plot\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe coffee data is dominated by small producers - with many populations and productions below 10 million people and pounds. The relationship between the variables is obscured by the clumping of data, which is caused by the massive outliers of Brazil (massive production) and India (massive population). A log-log plot (with log axes on the x and y) reveals a different perspective on this data. You must be careful, however, because log axes are not incredibly easy for most people to read, and the essentially everyone (including those that can read log-log scales) expects the scales on a plot to be linear unless explicitly warned otherwise.\n\n\nshow code for this result\ncoffee_plot + \n  scale_x_log10() +\n  scale_y_log10()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere are many more transformations the scale functions can perform (like a boxcox, or logit transform), and you can see a fuller list in the documentation of the scale_x_continuous function (or any of the continuous scale functions). Okay; bye, coffee data!\nWe can finally use scales functions to control the labels on the axes. I find this is particularly helpful if you have percentages on one axis. We have a proportion, which can be represented as a percentage.\n\n\nshow code for this result\np &lt;- p + scale_y_continuous(labels = scales::percent_format())\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Themes\nThe plot we have still doesn‚Äôt look quite like the reference plot we aimed toward. This can be explained by a difference in theme. The reference plot uses the minimal theme, whereas ours uses the default theme. We can ‚Äúfix‚Äù that by using the theme_minimal function.\n\n\nshow code for this result\np + theme_minimal()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can see the other default themes by seeing the help page for theme_minimal (i.e., ?theme_minimal, which is also the help page for the other built-in ggplot themes). My other favorite is theme_classic.\n\n\nshow code for this result\np + theme_classic()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nI also use theme_void, but typically only when making pie charts. theme_void removes most of the elements in the plot, including the x and y axes and gridlines.\n\n\nshow code for this result\np + theme_void()\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nshow code for this result\np &lt;- p + theme_minimal()\n\n\n\n\n4.2.4 Guides, Legends, and Text\nAlthough it makes no sense not to have it in this case, there are some cases in which you would like to remove the legend, which appears to the right of the plot by default. There are two ways to do this, using guides or using theme.\nguides is a function that is occasionally helpful for specifying which type of scale variables should be mapped to. You could set the guide to the color and/or shape arguments to ‚Äúnone‚Äù to remove one or both aspects of the legend.\n\n\nshow code for this result\np + guides(\n  color = \"none\"\n)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also remove the legend, instead of removing variables from the legend. You would do this using the legend.position argument in the theme function.\n\n\nshow code for this result\np + theme(legend.position = \"none\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou could also move the legend around using the legend.position argument.\n\n\nshow code for this result\np &lt;- p + theme(legend.position = \"top\")\n\np\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nFinally, you can use the theme function to make really specific changes to the plot, like changing the angle of the numbers of the y-axis\n\n\nshow code for this result\np + theme(axis.text.x = element_text(angle = -30),\n          axis.text.y = element_text(angle = 30))\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023. ‚ÄúData Visualization.‚Äù In R for Data Science, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData Visualization.‚Äù In R for Data Science, 1st ed. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "05_probability.html",
    "href": "05_probability.html",
    "title": "5¬† Probability Primer",
    "section": "",
    "text": "What is the law of large numbers?\nWhy are normal distributions important? What is the central limit theorem?\nWhat is a probability distribution? How do you write a function for a pdf and what are the properties of a normal distribution?\n\n\n\nshow code for this result\nsongs &lt;- read_csv(\n  file = \"data/Iconic_Songs_Dataset.csv\",\n  col_types = list(\n    Genre = \"factor\"\n  )\n) |&gt; \n  \n  # convert names to lowercase\n  janitor::clean_names() |&gt; \n  \n  # lump together genres will very little representation\n  mutate(genre = fct_lump(genre, n = 5)) |&gt; \n  \n  # add season variable\n  mutate(\n    season = case_when(\n      release_date &lt; \"2024-03-21\" ~ \"winter\",\n      release_date &lt; \"2024-06-21\" ~ \"spring\",\n      release_date &lt; \"2024-09-21\" ~ \"summer\",\n      release_date &lt; \"2024-12-21\" ~ \"fall\",\n      TRUE ~ \"winter\"\n    ),\n    season = factor(season)\n  ) |&gt; \n  \n  ####### linguistic variables\n  mutate(\n    length = str_length(title),\n    num_words = str_count(title, \"\\\\w+\"),\n    num_consonants = str_count(title, \"[^aeiou\\\\s]\"),\n    num_vowels = str_count(title, \"[aeiou]\")\n  )\n  \n\nglimpse(songs)\n\n\nRows: 100\nColumns: 12\n$ title          &lt;chr&gt; \"Blinding Lights\", \"Levitating\", \"Peaches\", \"Save Your ‚Ä¶\n$ artist         &lt;chr&gt; \"The Weeknd\", \"Dua Lipa\", \"Justin Bieber ft. Daniel Cae‚Ä¶\n$ genre          &lt;fct&gt; Pop, Pop, Pop, Pop, Pop, Pop, Pop, Pop, K-Pop, Pop, Pop‚Ä¶\n$ release_date   &lt;date&gt; 2024-01-05, 2024-01-14, 2024-02-12, 2024-02-18, 2024-0‚Ä¶\n$ duration       &lt;time&gt; 03:22:00, 03:23:00, 03:18:00, 03:36:00, 02:58:00, 02:2‚Ä¶\n$ album          &lt;chr&gt; \"After Hours\", \"Future Nostalgia\", \"Justice\", \"After Ho‚Ä¶\n$ popularity     &lt;dbl&gt; 95, 93, 91, 92, 94, 90, 89, 88, 96, 97, 85, 87, 89, 90,‚Ä¶\n$ season         &lt;fct&gt; winter, winter, winter, winter, winter, winter, winter,‚Ä¶\n$ length         &lt;int&gt; 15, 10, 7, 15, 8, 4, 12, 30, 6, 15, 13, 7, 17, 10, 10, ‚Ä¶\n$ num_words      &lt;int&gt; 2, 1, 1, 3, 3, 1, 3, 6, 1, 2, 2, 2, 3, 2, 3, 3, 2, 1, 3‚Ä¶\n$ num_consonants &lt;int&gt; 11, 6, 4, 7, 4, 3, 6, 16, 4, 9, 10, 3, 10, 6, 6, 8, 7, ‚Ä¶\n$ num_vowels     &lt;int&gt; 3, 4, 3, 6, 2, 1, 4, 9, 2, 5, 2, 3, 5, 3, 2, 4, 3, 2, 4‚Ä¶\n\n\nshow code for this result\nregression_data &lt;- songs |&gt; \n  select(\n    popularity,\n    genre,\n    duration,\n    season,\n    length,\n    num_words,\n    num_consonants,\n    num_vowels\n  )\n\n\n\n\nshow code for this result\nlin_reg &lt;- lm(popularity ~ ., data = regression_data)\nsummary(lin_reg)\n\n\n\nCall:\nlm(formula = popularity ~ ., data = regression_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.754 -2.111 -0.088  2.207  9.063 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    89.9019161  3.0441174  29.533   &lt;2e-16 ***\ngenreK-Pop      4.8031119  2.2018416   2.181   0.0319 *  \ngenreR&B       -1.5419138  2.2130330  -0.697   0.4878    \ngenreHip-Hop   -2.4326188  0.9989264  -2.435   0.0169 *  \ngenreReggaeton -0.8753986  1.9499158  -0.449   0.6546    \ngenreOther     -4.0473232  1.7902661  -2.261   0.0263 *  \nduration        0.0002057  0.0001794   1.147   0.2547    \nseasonspring   -1.5634697  1.1769945  -1.328   0.1876    \nseasonsummer    0.7545486  1.1617449   0.649   0.5177    \nseasonwinter   -1.1008813  1.1731980  -0.938   0.3507    \nlength          0.4036322  1.4068314   0.287   0.7749    \nnum_words       0.0107449  1.3898735   0.008   0.9938    \nnum_consonants -0.2916577  1.4478230  -0.201   0.8408    \nnum_vowels     -0.6175016  1.5473306  -0.399   0.6908    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.638 on 86 degrees of freedom\nMultiple R-squared:  0.2066,    Adjusted R-squared:  0.08663 \nF-statistic: 1.722 on 13 and 86 DF,  p-value: 0.07039\n\n\nshow code for this result\nlibrary(broom)\n\nlin_reg &lt;- lm(popularity ~ genre + duration + season, data = regression_data)\ntidy(lin_reg)\n\n\n\n  \n\n\n\n\n\nshow code for this result\ngenerate &lt;- function(n, beta, sigma, degree) {\n  tibble(\n    x = runif(n, -10, 10),\n    y = beta * (x ^ degree) + rnorm(n, 0, sigma)\n  )\n}\n\ngenerate(100, 10, 500, 2) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x,\n              method = \"lm\", \n              se = T,\n              color = \"red\") +\n  geom_smooth(formula = y ~ poly(x, 2),\n              method = \"lm\", \n              se = T,\n              color = \"green\") +\n  geom_smooth(formula = y ~ poly(x, 4),\n              method = \"lm\", \n              se = T,\n              color = \"blue\") +\n  geom_smooth(formula = y ~ poly(x, 10),\n              method = \"lm\", \n              se = T,\n              color = \"yellow\") +\n  theme_minimal()",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Probability Primer</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html",
    "href": "06_eugenics.html",
    "title": "6¬† Regression and Power",
    "section": "",
    "text": "6.1 The Technology of Correlations\nThe data used in this example were collected during the commission of this study: Pearson and Lee (1903). In the course of his work, Pearson was interested in collecting what he called anthropometric measurements of the population for a variety of purposes. To establish the heritability of physical characteristics (and to pit his own theory against Mendelian genetics), he collected family data. The data we will work with are 1078 measurements of a father and his adult son‚Äôs height. The small_data is a random sample of the data (\\(n=10\\)) for illustrative purposes.\nThe first step with any of this is to see it on a scatter plot. From the scatterplot, we see a visual correlation between the heights of father and son. Relatively taller fathers appear to have relatively taller sons.\nshow code for this result\n# pipe small_data into ggplot function as first argument (data)\nscatter_plot &lt;- small_data |&gt; \n  ggplot(aes(x = father, y = son)) +\n  geom_point() +\n  \n  labs(title = \"Father-Son Height Correlation\",\n       x = \"Father's Height (inches)\",\n       y = \"Son's Height (inches)\") +\n  \n  theme_minimal()\n\nscatter_plot\nThe idea of regression is to find the straight line that best represents this correlation that we see in the data. The line is defined by two parameters: the slope and the intercept. Completely random, but let‚Äôs guess and say the slope is 0.78 and the intercept is 16. We can plot this line on the scatter plot to see how well it fits. The intuition I was encouraged to develop here is that there should be an equal number of points above and below the line, which is-in fact-the case.\nshow code for this result\nscatter_plot &lt;- scatter_plot + \n  geom_abline(intercept = 16, slope = 0.78, color = \"blue\", linetype = \"dashed\")\n\nscatter_plot\nHow good is this line? Middle school algebra says perfect, but if you had to put a number on the goodness of the fit of this line to this data, how would you do that? Here is an idea. The goodness of the line depends on how close all the points are to it; let‚Äôs call this distance the deviation of a point from the line.\nTo compute the deviation from the line, we subtract the predicted value of \\(y\\) from the actual value of \\(y\\). In this case, let‚Äôs predict the son‚Äôs height from the father‚Äôs height using the line we just drew. The predicted value of \\(y\\) is given by the equation of the line: \\(y = mx + b\\), where \\(m = 0.78\\) and \\(b = 16\\).\nshow code for this result\npredict_height &lt;- function(father) {\n  0.78 * father + 16\n}\npredict_height(62)\n\n\n[1] 64.36\nNow we can use our height predicting function to compute the predicted height of each father‚Äôs son based on the father‚Äôs height. With the prediction and the actual observation, we can compute the deviation of each point from the line.\nshow code for this result\nsmall_data &lt;- small_data |&gt; \n  mutate(pred_son = predict_height(father)) |&gt; \n  mutate(vert_dev = son - pred_son)\n\nsmall_data\nTo bring us back to the plot, we can add error lines to the scatter plot to show the deviation of each point from the line. The red lines show the deviation of each point from the line. The length of the line is the deviation of the son‚Äôs height from the predicted height.\nshow code for this result\nscatter_plot + \n  geom_segment(data = small_data, aes(x = father, \n                                      y = pred_son, \n                                      xend = father, \n                                      yend = son), \n               color = \"red\", \n               alpha = 0.5)\nIt is important to note that this is only one way to think of a deviation. We have drawn our deviation lines vertically, but we could have drawn them horizontally by predicting the fathers height from the sons.\nshow code for this result\npredict_father_height &lt;- function(son) {\n  (son - 16) / 0.78\n}\n\nsmall_data &lt;- small_data |&gt; \n  mutate(pred_father = predict_father_height(son)) |&gt; \n  mutate(horiz_dev = father - pred_father)\n\nscatter_plot + \n  geom_segment(data = small_data, aes(x = pred_father, \n                                      y = son, \n                                      xend = father, \n                                      yend = son), \n               color = \"red\", \n               alpha = 0.5)\nIf we wanted to be really pretentious, we could use orthogonal deviations that are perpendicular to our line of best fit.\nshow code for this result\n# linear algebra brings us these numbers\n# I had to solve a system of two equations to get them\n# -0.78 * x + y = 16\n# (1/0.78) * x + y = b\nsmall_data &lt;- small_data |&gt; \n  mutate(b = son + (100/78) * father,\n         xend = -7.76 + 0.485 * b,\n         yend = 9.952 + 0.378 * b)\n\nscatter_plot +\n  geom_segment(data = small_data, aes(x = father, \n                                      y = son, \n                                      xend = xend, \n                                      yend = yend), \n               color = \"red\", \n               alpha = 0.5)\nEach of these methods (exemplified by the little red lines) is a different and perfectly valid way to compute the deviation of a point from this line. There are also other valid ways to do so that I have not covered here (e.g., Manhattan or city-block distance). I used each of these three methods (vertical, horizontal, and orthoganal, which is more complicated) to compute the deviation of each point from the line and stored them in deviations.\nshow code for this result\ndeviations &lt;- small_data |&gt; \n  mutate(\n    delta_x = father - xend,\n    delta_y = son - yend,\n    orthog_dev = case_when(\n      delta_x &gt; 0 & delta_y &lt; 0 ~ -sqrt(delta_x^2 + delta_y^2),\n      delta_x &lt; 0 & delta_y &gt; 0 ~ sqrt(delta_x^2 + delta_y^2)\n    )\n  ) |&gt; \n  # round all deviations to 2 decimal places\n  mutate_at(vars(contains(\"dev\")), ~round(., 2)) |&gt;\n  select(father, son, vert_dev, horiz_dev, orthog_dev)\n\ndeviations\nHow are we going to use these deviations to measure the goodness of the line? Shall we try to sum them up?\nshow code for this result\ndeviations |&gt; \n  summarise(\n    sum_vert_dev = sum(vert_dev),\n    sum_horiz_dev = sum(horiz_dev),\n    sum_orthog_dev = sum(orthog_dev)\n  )\nIt is very uncommon to sum up deviations. Because deviations can be negative or positive, they can cancel each other out. Theoretically, the sum of deviations from a line of best fit should always be zero (although, as you can see from the table above, it is not always the case that they sum to exactly 0). Typically, statisticians will square the deviations before adding them to avoid exactly this problem. The resultant sum of squared deviations is called the residual sum of squares (RSS; residual because they are deviations from the ‚Äúmodel‚Äù which is the line of best fit).\nshow code for this result\ndeviations |&gt; \n  summarise(\n    rss_vert = sum(vert_dev^2),\n    rss_horiz = sum(horiz_dev^2),\n    rss_orthog = sum(orthog_dev^2)\n  )",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#the-technology-of-correlations",
    "href": "06_eugenics.html#the-technology-of-correlations",
    "title": "6¬† Regression and Power",
    "section": "",
    "text": "6.1.1 Least Squares Regression\nThe problem of fitting a line of best fit shakes out as follows. We will define an error function that measures the goodness of fit of a line. This error function will be the sum of squared deviations from the line, using whichever definition of deviation we prefer (conventionally, vertical deviation is used).\nThe framing of error in this way also allows us to think of this problem as an optimization problem. A residual sum of squares of 0 will indicate a perfect fit (our goal), and higher values indicate progressively worse fit. Framing this as an optimization problem allows us to use the tools of calculus together with those of linear algebra to find the line that minimizes the error function.\nWe will define a line of best fit as the line that results in the smallest possible value of the error function. When we use the sum of squared deviations as an error function, this line is called the least squares regression line. We have to define the line in these terms because in linear algebra jargon we are dealing with a system of equations that is over-determined. We have more data points than there are variables, and there is no straight line that will pass through all of them.\nThe mathematical problem of least squares has existed for a long time, and was solved by two men in the early 19th century. The first to solve it was Gauss, but the first to publish the solution was Adrien-Marie Legendre (in 1805). The invention of linear regression is often attributed to Francis Galton, however, because it was he who coined the term ‚Äúregression‚Äù and popularized the method.\nThe least squares method models the independent variable \\(y\\) as a linear function of the dependent variable \\(x\\) with an additional error term, commonly denoted by either \\(e\\) or the greek letter epsilon \\(\\epsilon\\). The error term is the difference between the observed value of the dependent variable and the value predicted by the model. The model can be written as follows (where x is father‚Äôs height and y is that of the son):\n\\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\n\\(\\beta_0\\) in this equation is the intercept (known as just \\(b\\) in high school algebra), and \\(\\beta_1\\) is the coefficient of \\(x\\) (known as \\(m\\) in high school algebra). The goal of the method is to find a value of \\(\\beta_0\\) and \\(\\beta_1\\) that minimizes the sum of squared errors.\nTo compute the value of these coefficients, you can use the lm function to perform least squares regression.\n\n\nshow code for this result\nlinear_model &lt;- lm(son ~ father, data = small_data)\nsummary(linear_model)\n\n\n\nCall:\nlm(formula = son ~ father, data = small_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.330 -1.903 -0.422  1.787  3.491 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  16.1539    20.1612   0.801   0.4461  \nfather        0.7827     0.2938   2.665   0.0286 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.266 on 8 degrees of freedom\nMultiple R-squared:  0.4702,    Adjusted R-squared:  0.404 \nF-statistic:   7.1 on 1 and 8 DF,  p-value: 0.0286\n\n\nI often find it helpful to ‚Äútidy‚Äù this output using the broom package, primarily so that I can see the confidence intervals for the coefficients, which are far easier to interpret than the p-values (which Pearson would have relied on).\n\n\nshow code for this result\nbroom::tidy(linear_model, conf.int = TRUE, conf.level = 0.95)\n\n\n\n  \n\n\n\nThe estimate refers to the coefficient (one of the \\(\\beta\\) from the model equation), and the std.error is the standard error of the coefficient estimate. Using this information, the a t-statistic is calculated (statistic) from which the p.value is derived. The conf.low and conf.high values are the lower and upper bounds of the 95% confidence interval for the coefficient.\nThe t-test operates on the null hypothesis that the coefficient is equal to 0. Null hypothesis significance testing requires that we set a threshold for the type-1 error rate (the rate at which we will falsely reject the null hypothesis). Typically, social scientists use a maximum type-1 error rate of 0.05. They reject the null hypothesis and declare that the coefficient is significantly different from 0 if the p-value is less than 0.05, as it is in this case. The p-value is difficult to interpret and does not provide any information about the size or practical significance of the effect.\nThe conf.level argument is not involved in the t-test, but it is involved in the computation of the confidence interval. If you increase the conf.level, you will make the confidence interval wider, and if you decrease it, you will make the interval narrower. The confidence interval is easier to interpret than the p-value, but its interpretation is still a little odd. If we repeated the same experiment many times (enough times to allow asymptotic theories like the CLT to kick in), we would expect the true value of the coefficient to fall within the confidence interval 95% of the time. This is a little different from saying that we are 95% confident that the true value of the coefficient falls within the interval. The latter interpretation is a little more intuitive, but it is significantly less correct.\nThe following plot might aid in understanding the confidence interval. Performing a linear regression for all of our data results in a coefficient estimate of 0.51. If we take 40 bootstrap samples (i.e., drawing a sample from the data with replacement 40 times) and construct a confidence interval for each, we should find that the true value of the coefficient (let‚Äôs just assume 0.51, but really we don‚Äôt know) will fall outside of the confidence interval about 2 times.\n\n\nshow code for this result\nfull_model &lt;- lm(son ~ father, data = data)\ncoef(full_model)\n\n\n(Intercept)      father \n 33.8928005   0.5140059 \n\n\n\n\n\n\n\n\n\n\n\nThe coefficient of the simple linear regression model should be interpreted as a slope. For every one unit increase in the father‚Äôs height, there is a 0.51 unit increase in the son‚Äôs height on average. Also of interest is how much of the variance the model is able to explain. This is given by the \\(R^2\\) value, which is 0.25 in this case. This means that 25% of the variability in the son‚Äôs height can be explained by the father‚Äôs height. \\(R^2\\) is sometimes called the coefficient of determination, and it ranges from 0 to 1. Higher \\(R^2\\) values indicate that the model is better at explaining the variability in the dependent variable. You can get the \\(R^2\\) from through the summary function, the glance function (of the broom package), or by calculating it yourself. When the data is moved closer to the regression line, \\(R^2\\) will increase; when data is farther away from the line, \\(R^2\\) will decrease.\n\n\nshow code for this result\nbroom::glance(full_model)\n\n\n\n  \n\n\n\nshow code for this result\nrss &lt;- sum(residuals(full_model)^2)\ntss &lt;- sum((data$son - mean(data$son))^2)\nr_squared &lt;- 1 - rss/tss\nr_squared\n\n\n[1] 0.251164\n\n\nshow code for this result\ncor(data$father, data$son)^2\n\n\n[1] 0.251164\n\n\n\n\n\n\n\n\nAssumptions\n\n\n\n\nlinearity\n\nLinear regression and correlation are only appropriate when the relationship between the two variables is linear. If the relationship is not linear, the correlation coefficient will not accurately reflect the strength of the relationship and will generally be meaningless. In the following data, I find a weak negative effect of x on y, and a linear regression shows that the coefficient is not significantly different from 0.\n\n\nshow code for this result\ndf &lt;- datasauRus::datasaurus_dozen |&gt; filter(dataset == \"dino\")\ncor(df$x, df$y)\n\n\n[1] -0.06447185\n\n\nshow code for this result\nlm(y ~ x, data = df) |&gt; broom::tidy()\n\n\n\n  \n\n\n\nThis is completely meaningless, however. If I performed this regression and interpreted it as ‚Äúa one unit increase in x leads to a 0.1 unit decrease in y, on average‚Äù I would be lying (i.e., making a factual claim without evidence). The data is not linear, as you can see if you plot it. You must always plot your data before performing a correlation or regression analysis.\n\n\nshow code for this result\nggplot(data = df, aes(x, y)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, \n              method = \"lm\", \n              se = FALSE, \n              linetype = \"dashed\") +\n  labs(title = \"Do not use linear regression for non-linear data\",\n       subtitle = \"it doesn't mean anything\",\n       x = \"\",\n       y = \"\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nindependence\nhomoscedasticity\nnormality\n\n\n\n\n\n6.1.2 Standardized Correlation Coefficients\n\n\nshow code for this result\ncov(small_data$father, small_data$son)\n\n\n[1] 5.177111\n\n\nshow code for this result\ncor(small_data$father, small_data$son)\n\n\n[1] 0.6857023",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#cancelling-karl-pearson",
    "href": "06_eugenics.html#cancelling-karl-pearson",
    "title": "6¬† Regression and Power",
    "section": "7.1 Cancelling Karl Pearson",
    "text": "7.1 Cancelling Karl Pearson\nAs previously mentioned, correlation is excessively important to frequentist statistics. It is used to determine the strength and direction of a linear relationship between two variables. I learned many of the statistical techniques I will teach you later on (linear and logistic regression, ANOVA, principal component analysis, it goes on and on) in a class called ‚ÄúCorrelational Techniques‚Äù.\nThe University College London has a Pearson archive from which some of the following information comes. (Other info is cited or editorial.)\nPearson was born in London in 1957, and graduate from Cambridge with a degree in mathematics at the age of 22 (in 1879). Pearson was encouraged by, collaborated with, and wrote a biography of Francis Galton. If you‚Äôll recall, Galton popularized the median and also eugenics. Perason and Galton also collaborated frequently with Walter Weldon, who was a zoologist. He (and the three of them more generally) believed that evolution was an essentially statistical phenomena. Collectively, they founded the journal Biometrika in 1901.\n\nA single individual may have a variation which fits it to survive, but unless that variation appears in many individuals, or unless that individual increases and multiples without loss of the useful variation up to comparatively great numbers‚Äìshortly, until the fit type of life because a mass-phenomenon, it cannot be an effective factor in evolution. The moment this point is grasped, then whether we hold variation to be continuous or discontinuous in magnitude, to be slow or sudden in time, we recognize that the problem of evolution is a problem in statistics, in the vital statistics of populations. Whatever views we hold on selection, inheritance, or fertility, we must ultimately turn to the mathematics of large numbers, to the theory of mass-phenomena, to interpret safely our observations. As we cannot follow the growth of nations without the statistics of birth, death, duration of life, marriage and fertility, so it is impossible to follow the changes in any type of life without its vital statistics. The evolutionist has to become in the widest sense of the words a registrar-general for all forms of life.\n(IISpiritBiometrika1901?), emphasis added\n\nIn the spirit of Hilbert and Friends (see ?sec-early-19th-century-the-advent-of-computing), these three were interested in formalizing the theory of evolution in mathematical notation. We must admit, as we did with the mathematicians, that their aim is incredibly ambitious. They believe that there is a mathematical truth that defines and controls life in the broadest and most encompassing understanding of that term. Discovering that truth (which I do not believe exists) was their aim. This formulation of life in general, requiring de-individualized consideration at population levels, was a profitable ideology (episteme) under which to enact control over individual bodies and lives through public policy and social convention (Foucault 1978). This framework enables statements about ‚Äútraits‚Äù (a euphemism for the people with those traits) and their disastrous effect on the ‚Äúpopulation‚Äù (or even ‚Äúlife‚Äù) as a whole, naturally leading to the conclusion that removing the ‚Äútraits‚Äù (which are, again, people) from the population would make that population better and more fit, as assessed with statistics (like Galton‚Äôs median). These men express an excessive trust in mathematics as an explanation for even very complex (and random!) biological processes and traits (like sexual orientation or ‚Äúduration of life‚Äù, traits so complex and mutable in their formation that I take the them to be unpredictable, certainly not in any mathematically rigorous and informative way).\n\nThese words‚Ä¶ may well serve as a motto for Biometrika and for all biometricians: I have no faith in anything short of actual measurement and the Rule of Three1.\n(IISpiritBiometrika1901?), emphasis original\n\nI‚Äôm going to pull a Foucault and analyze this journal, and the philosophies expressed in its early issues, as a technology: a system that is shaped by its socio-historical context and its function. The philosophy of Biometrika reeks of the ‚Äúgo fast and break things‚Äù approach that is all to familiar as mottos of the plutocracy (Elon Musk, fossil fuel magnates, bankers that caused the collapse of the global housing market, the AI people who continue to develop their technology despite their own protests/marketing that it is powerful enough to pose a threat to the human race, etc.). The tool these technocrats (scientists) were going to abuse was the correlation, and boy did they ever abuse it! Not only did correlation entail causation, but in the pages of Biometrika, every correlation was subject to interpretation as having a bio-evolutionary cause.\nThe editors surely saw themselves as progressive, and they acted in surprisingly progressive ways. In the first volume, for example, there is a reiteration that researchers should make their data available:\n\nI have begun to think that no one ought to publish biometric results, without lodging a well arranged and well bound manuscript copy of all his data, in some place where it should be accessible, under reasonable restrictions, to those who desire to verify his work.\n(galtonBIOMETRY1901?)\n\nThe guards against any critical verification actually taking place were three fold. Firstly, the editors intend the journal to merge biology and mathematics, producing article that will be unfamiliar to practitioners of each. Those practitioners would have to turn to the experts (in many cases, the editors of the journal) to learn how to misinterpret the data in the same way. Secondly, because ‚Äúthe new methods occupy an altogether higher plan than that in which ordinary statistics and simple averages move and have their being‚Ä¶ the arithmetic they require is laborious, and the mathematical investigations on which the arithmetic rests are difficult reading even for experts.‚Äù The inability to understand and interpret the incredibly complicated methods on which their conclusions rested led many authors in Biometrika (and certainly many more readers) to make false or overly ambitious (i.e., ill-supported) conclusions, many of which continue to circulate to this day under the guise of scientific fact. Thirdly, the level of mathematical understanding required to challenge the conclusions (and the failing willingness of those at the helm of the institution to listen to those who didn‚Äôt look like them) resulted in only a very homogeneous and powerful group of scientists being left to perform ‚Äúverification.‚Äù That‚Äôs not to say that there was no one left to criticize biometry; there certainly were, not that the editors of Biometrika (or at least Galton) cared:\n\nIt is not in the least my intention to insinuate that Biometry might be served by any modern authority in so rough a fashion [as the Royal Society treated the alleged founders of geology], but I offer the anecdote as forcible evidence that a new science cannot depend on a welcome from the followers of older ones, and to confirm former conclusions that it is advisable to establish a special Journal of Biometry.\n(galtonBIOMETRY1901?)\n\nThis early quote hints at a period before biometrics had the serious legitimacy it would later acquire. Biometrika would not only be a tool to increase the legitimacy of this growing field, but also to potentially keep it afloat if its favor began to tank. This must have seemed like a venue of considerable importance, then. Biometrics (and therefore biometrika) often made opposite assumptions to the dominant theory of Mendelian heredity (yet another way in which this theory was progressive for the time).\nPearson was interested in using science to improve mankind. This is a fundamentally progressive objective, and he rejected the utility of phrenology (which he refers to as anthropometry).\n\nI am afraid I am a scientific heretic‚Äìan outcast from the true orthodox faith‚ÄìI do not believe in science for its own sake. I believe only in science for man‚Äôs sake‚Ä¶ the progress of mankind in its present stage depends on characters wholly different from those which have so largely occupied the anthropologist‚Äôs attention. Seizing the superficial and easy to observe, he has let slip the more subtle and elusive qualities on which progress, on which national fitness for this or that task essentially depends. The pulse-tracing, the reaction-time, the mental age of men under his control are for more important to the commanding officer‚Äìnay, I will add, to the employer of labor‚Äìthan any record of span, of head-measurement or pigmentation categories. The psycho-physical and psycho-physiological characters are of far greater weight in the struggle of nations to-day than the superficial measurements of man‚Äôs body. Physique, in the fullest sense, still counts something still, bit it is physique as measured by health, not by stature or eye-color. But character, strength of will, mental quickness count more, and if anthropometry is to be useful to the state it must turn from these rusty old weapons, these measurements of stature and records of eye-color to more certain appreciations of bodily health and mental aptitude‚Äìto what we may term ‚Äòvigorimetry‚Äô and to psychometry.\nPearson (1920)\n\nTo return briefly to the notion of Foucaultian bio-power (for which I have given only a meager explanation), we can see that Karl Pearson‚Äôs explicit aim, at least circa 1920 was that his work would be integrated into the work of the state; that his statistics (and his eugenics) become the knowledge against which to judge the value of individual human lives, establishing the cut-off under which one was liable to be sterilized, killed, or subject to such a thorough judicial and scientific examination that one wished they had been killed. Pearson is really quite explicit about this:\n\nWe have to make anthropology a wise counsellor of the state, and this means a counsellor in political matters, in commercial matters, and in social matters.\nPearson (1920)\n\nPearson here asks not only for anthropology to be reformed in his image (by ditching all the soft human shit and turning to quantification and numbers instead), and then for states to ruthlessly and expansively apply this science to (control their citizenry in hopes that they might) make the population more evolutionarily fit (and productive to the capitalist system). He even goes so far as to repeatedly call anthropology (as he envisioned it) the ‚Äúqueen of the sciences‚Äù (Pearson 1920)2. He sees state use of anthropology as necessary to progress the species (and guard against defeat in armed conflict).\n\nThen, I think, you will agree with me that rightly or wrongly there is a conviction spreading in Germany that the war arose and that the war was lost because a nation of professed thinkers had studied all sciences, but had omitted to study aptly the science of man. And in a certain sense this is an absolutely correct conviction, for if the science of man stood where we may hope it will stand in the dim and distant future, man would from the past and the surrounding present have some grasp of future evolution, and so have a greater chance of guiding its controllable factors.\nPearson (1920)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#eugenics",
    "href": "06_eugenics.html#eugenics",
    "title": "6¬† Regression and Power",
    "section": "7.2 Eugenics",
    "text": "7.2 Eugenics\nEugenics relies on biological determinism.\n\nHe has never grasped that the man of to-day is precisely what heredity and his genealogy, his past history and his prehistory, have made him. He does not recognise that it is impossible to build your man for the future until you have studies the origins of his physical and mental constitution‚Ä¶ Man has not a plastic mind and body which the enthusiastic reformer can at will mould to the model of his golden age ideals. He has taken thousands of years to grow into what he is, and only by like processes of evolution‚Äìintensified and speeded up, if we work consciously and with full knowledge of the past‚Äìcan we build his future.\nPearson (1920)\n\nPearson took advantage of socio-political circumstances (like racialized grief after the killing of millions of white boys in WW1) to create moral and ontological panics.\n\nWe have seen a large part of the youth who were best fitted mentally and physically to be parents of feauture generations perish throughout Europe: the dysgenic effect of this slaughter will show itself each twenty to twenty-five years for centuries to come in the census returns of half the countries of the world.\nPearson (1920)\n\nViewing it in socio-historical context, especially considering the incredibly innovation in destructive weapons over the second half of the 19th century, Viewing it in socio-historical context, especially considering the incredibly innovation in destructive weapons over the second half of the 19th century, the case for the inherent wickedness (or the inherent weakness of humanity in resistance to wickedness) was getting easier and easier to evidence, as violence became more heinous and more immediate (through photography, for example).\nEugenics was thoroughly connected to capitalism, and Pearson‚Äôs vision of it was that it would touch every aspect of society, optimizing the human condition (and surplus profits) as it progressed.\n\nWide, however, as is the anthropometric material in our universities and public schools, it only touches a section of the population. The modern anthropologist has to go further; he has to enter the doors of the primary schools; he has to study the general population in all its castes, its craftsmen, and its sedentary workers. Anthropology has to be useful to commerce and to the State, not only in association with foreign races, but still more in the selection of the right men and women for the staff of factory, mine, office, and transport. The selection of workmen to-day by what is too often a rough trial and discharge method is one of the wasteful factors of production.\n(Pearson 1920)\nBut the anthropologist, if he is to advance his science and emphasise its service to the State, must pass beyond the university, the school, and the factory. He must study what makes for wastage in our present loosely organized socitey; he must ingestigate the material provided by the reformatory, prison, asylums for the insane and mentally defective; he must carry his researches into the inebriate home, the sanatorium, and the hospital, side by side with his medical collaborator.\nThe future lies with the nation that most truly plans for the future, that studies most accurately the factors which will improve the racial qualities of future generations either physically or mentally. Is anthropology to lie outside of this essential function of the science of man? If I understand the recent manifesto of the German anthropologists, they are determined that it shall not be so. The war is at an end, but the critical time is with us again, I sadly fear, in twenty to thirty years. How will the States of Europe stand then? It depends to not little extent on how each of them may be cultivated the science of man and applied its teaching to the improvement of national physique and mentality. Let us take care that our nation is not last in this legitimate rivalry. The organisation of existing human society with a view to its future welfare is the crowning task of the science of man; it need the keenest-minded investigators, the most stringent technique, and the utmost sympathy from all classes of society itself.\nPearson (1920)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#eugenic-statistics-today",
    "href": "06_eugenics.html#eugenic-statistics-today",
    "title": "6¬† Regression and Power",
    "section": "7.3 Eugenic Statistics Today",
    "text": "7.3 Eugenic Statistics Today\nHow are we, as people who want to do statistics, to make sense of this incredibly alarming information about the original purpose of the tools we are going to use. How do we prevent falling into the same trap that the eugenicists fell, believing contentedly that we are contributing to the progress of humanity while, in fact, our work is just a collection of our preconceived notions that could wind up causing destruction and ruin at a scale we couldn‚Äôt imagine in our wildest dreams?\nIn my psychology lectures, Pearson is a eugenicist, and that makes him a bad person (fair enough, honestly). This causes a paradox because no one wants to stop using the methods that Pearson and his contemporaries popularized and - in many cases - invented. Those things are good ideas, but they come from a bad person. How could that be?\nDoes this mean that bad persons can have good ideas? That Pearson wasn‚Äôt actually a bad person, or that he was more morally complex than brief presentation by my psychology professors suggests?\nI don‚Äôt know how people justify the use of these methods to themselves. I don‚Äôt know how they expect that they will use the same methods in similar ways without reproducing and enacting the functions these tools were designed for. My speculation is as follows.\nThis material is often difficult to read (and it is common to find sensational articles and quotes that are truly abhorrent, and which I mostly did not include here). It causes you to question humanity by demonstrating the sorts of things we‚Äôre capable of thinking and doing, and the steps we are willing to take to ignore the harm our thoughts and actions have on others, including to reanalyze that harm as benefit (or just fit, if you‚Äôre a eugenicist). Because that reading is so uncomfortable, and there‚Äôs really no productive advantage to do it, (and also sometimes good sources can be difficult to find,) a lot of people just don‚Äôt.\nThe problem then becomes ignorance. If no one knows and talks about these things, we should only expect that scientists are going to go around licencing their work as ethically acceptable because it is, or at least sometimes looks, progressive. They, like Pearson, are trying to improve the condition of some population of people through acquisition of knowledge.\nScientists don‚Äôt use Pearson‚Äôs tools because they do the best job at reflecting the true nature of the world. Pearson‚Äôs tools serve a useful purpose, but this is not it. To the extent correlational statistics represent anything faithfully, it is the socio-historical context of their development and use. I would argue that these tools are such useful tools in the human sciences because the assumptions and philosophies behind the tools are the same philosophies and assumptions that limit human action in the real world, either through social convention, government policy, or some other effect of social power relations.\nThese tools remain useful for control of populations and individual lives because the philosophies underlying these tools are thoroughly dispersed throughout the social order. Their use has expanded beyond the notion bio-power, expanding now to a politics of attention, as well as various other innovations. By politics of attention, I mean to point out that correlational statistics (how similar is person x‚Äôs watch history to person y) form the foundation of many of the algorithmic tools that are used to control human attention (and thus thought and emotion) on digital platforms of all sorts today.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#notes-from-pearsons-book-about-states",
    "href": "06_eugenics.html#notes-from-pearsons-book-about-states",
    "title": "6¬† Regression and Power",
    "section": "7.4 Notes from Pearson‚Äôs Book about States",
    "text": "7.4 Notes from Pearson‚Äôs Book about States\nThis book (Pearson 1919)!\n\n‚ÄúIt is little wonder, perhaps, that the first investigators in this new field [biology] went widely astray. They extended without due thought fascinating biological hypotheses to the case of man. They found the struggle of individual against individual in many vital fields, and they extended the survival of the fitter as a governing principle to all individual life within human communities; they did not stay to inquire why or how communities had themselves come into existence; they neglected the suggestions of the hive and the herd, and reached (as in the cases of both Spencer and Huxley) fallacious conclusions as to the functions of the state and the sources of social conduct. In short, they over-emphasized the intra-racial struggle, and under-emphasized the inter-racial contest, as factors producing and developing the political and moral characteristics in man.‚Äù (2)\ndiscussion of why we Darwinian genetics applies (assumptions about the human race and then support of those assumptions with argument) - pages 3-4\n\n(c) - death is 50-80% heritable essentially - repetition from study in volume 1 issue 1 or 2 or Biometrika\n‚ÄúWe are forced, then, to the conclusion that the Darwinian theory in the case of mankind is a law and not a‚Äùplausible hypothesis.‚Äù (4)\n\nlook how great Romans, Hebrews, Greeks, and Europeans are (5-6)\nPearson saw the population as a biological entity of sorts, and spoke about it with florid language:\n\nThe struggle of nations is the commonplace of history; but the realization that this struggle is a factor in human development,‚Äìthat big battalions or an armada are not sufficient insurance for success in it, but that organization and intelligence in every function of national life are requisite for victory,‚Äìthis is the special truth that dawned upon us at the end of the nineteenth century. Formerly territory was blindly seized, trade routes and commercial markets blindly opened or controlled, manufacturing processes and means of transit developed or not, according as they might seem profitable or not to individuals. The bearing of these things and a multitude of others‚Äìsuch as the physique of the nation, the skill of its craftsmen, the intelligence of its trade-leaders, the activity of its educators, the organization and preservation of its material resources‚Äìwas unrecognized in their relation to national fitness for the international struggle. The politician could tell the nation that it must have more ships or more rifles for the national safety, or he could emphasize the importance of the ‚Äúopen door‚Äù for national welfare, but he did not provide for the intelligent building of the ships, the intelligent sighting of the rifles, the intelligent training of the merchants who were to enter the open food amid the great international crush to get inside. He did not see that ultimately the training of even the apparently most insignificant workers in the community, the fitness for its purpose of the simplest manufacturing or agricultural process, may be vital to a nation in the evenly balanced contest of modern civilization. To stand still‚Äìfor a moment to depend only on the possession of material resources, of the existing trade routes, or of means of transit‚Äìis to lose points in the game. Where all are pressing forward, not to advance is to fall behind.‚Äù (6-7)\n\nfactors of national success (7)\n\nthe physical powers:\n\nvital: fertility rates, ‚Äúhealth and sanitation, the energy, vigour, and and absolute strength of individuals‚Äù\nmaterial: wealth in resources (minerals, energy sources, water resources including docks)\n‚Äúequipment: the power to seize and the power to hold‚Äù\n\nthe mental powers:\n\n‚Äúcarry[ing] out mechanical work quickly and effectively‚Äù\ndiscovery and imagination\nincitement and stimulation\n‚Äúmorale and patriotism‚Äù\nendurance\ntendency away from ‚Äúepochs of national hysteria‚Äù\n‚Äúfollow[ing] for years definite policies with only future profit in view‚Äù\ngoverning effectively\n\nin lieu of the power to govern effectively, the power to ‚Äúultimately assimilate divergent racial groups. (the tell is here)\n\n\n\n‚ÄúBrute force, strength and bravery, material wealth, have in turn been dominant in the state; to-morrow will be marked by the dominance of intelligence. The most intelligent nations will be victorious in the struggle; and it befits each state that would be great to-morrow as well as to-day to educate and organize itself, from the statesman at the top to the plough-boys and factory hands at the basis. In the future it will not be possible either to organize and lead a nation or to make cheese effectively without training‚Äìwithout a knowledge of what science has to say about men or milk.‚Äù (8-9)\n‚ÄúCaste and class may be exaggerated so much that they do far more harm than good, but to a certain extent they may serve for differentiating workers within the community. the nation stands equally in need of its ploughmen, its craftsmen, its traders, its brain-workers, and its leaders; and it is desirable to have some preliminary classification of what work an individual is best suited for.‚Äù (9)\nsimping for caste systems (9-10)\n\n‚ÄúIt is cruel to the individual, it serves no social purpose, to drag a man of only moderate intellectual power from the hand-working to the brain-working group; yet this seems too often the result of the present system. If there be a moderately capable worker, the state should strive, in the first place, that he should be trained to better craftsmanship. Do not let it assume that he will turn out a Faraday [(very successful scientist)] because he shows some relative capacity. In at least nine cases out of ten disappointment will be in store for the state if it does. Let there be a ladder from class to class, and occupation to occupation, but let it not be a very easy ladder to climb; great ability will get up it, and that is all that is socially advantageous. We have to remember, for example, that the middle class in England, which stands there for intellectual culture and brain-work, is the product of generations of selection from other classes and of in-marriage. A hundred men of this class, quite apart from training and tradition, will provide a greater percentage of men capable of doing brain-work, than a hundred men from the farming class, or a hundred craftsmen.‚Äù (10)\n(11) - current class system is not ‚Äúa mere historical anomaly; it is largely the result of long-continued selection, economically differentiating the community into classes roughly fitted to certain types of work.‚Äù\n\neducation must be specified for each class of workers\n\n‚ÄúMake it easy for the Michael Faradays to climb, but only for such as he was; the increase of the intellectual proletariat is a sign not of effeiciency but chaos in national education.‚Äù (12)\n\n‚ÄúThe statesmen of the old school, blamelessly ignorant of the laws of national development, were inclined to look upon race-progress as due to mighty forces beyond human control, and thus to believe that executive and legislature could do little to make or mar national welfare. But as we learn to understand better the laws ruling living organisms, our appreciation of the factors in human history changes: man cannot modify the law of gravitation, but he can make its effects subserve his own ends; and this is equally true of the laws which rule organic and inorganic material.‚Äù\n\n‚Äúwithout proper selection or fit training the statesmen of the oligarchy may forget inter-racial competition under the bias of class interest, and the intra-racial dominance of a caste will become the chief objective of a false statecraft.‚Äù (14)\n\n‚ÄúHistorical evolution has left most civilized nations, after a rough and tumble experience, with a democratic government more or less tempered by oligarchic and autocratic institutions. This may be the best practical solution of the problem in the present stage of national development, but such a system is terribly cumbersome in its processes for ensuring that the keenest brains and the best organizing power of individuals shall be secured as the brains and the organizing power of the nation at large. If the best trained, the most intelligent community is destined to be the surviving type of the present century, then the cry must not only be: Educate your democracy! but also: Select and train your aristocracy for statecraft!‚Äù (15)\n‚Äúthere is no small doubt that we safely may assume that all qualities in man are inherited, and inherited at such a rate that very few‚Äìtwo to four‚Äìgenerations suffice for selection to produce a class breeding true to itself, then the selection of an aristocracy even by the rough process of ennobling great ability or great wealth (acquired by the owner) is intelligible.‚Äù\nneed to train statesmen (16, again!). What kinds of statesmen, you ask?\n\n‚ÄúThere should be one school at least where colonial institutions, ambitions, and developments are studied and appreciated; where national customs, racial prejudices, the foreign press, its powers and limitations, are calmly, and apart from political intrigue, investigated and weight in the balance; where the students‚Äô own nation, its comparative power and influence, its morale, and its policy are dealt with in an atmosphere comparatively free from party strife, and at an age when the mental judgement has not had its roadway worn into ruts by the continual traffic of men and affairs.‚Äù (17)\n‚ÄúUnless we have the statesman of insight, who recognizes that every function of the state, every phase of national life, has a theory of its own; that there is a right way and a wrong way of conducting all state business, whether it be concerned with the wealth, the physique, the intellectual efficiency, or the morale of a nation;‚Äìwe cannot place knowledge‚Äìscience in its broadest and truest sense‚Äìin its rightful position of consultant alongside executive. We must have stored knowledge, science theoretical or empirical, at the service of the state for the ordinary routine of every department of national activity.‚Äù (18)\n\n‚ÄúReadiness for pioneer-work is one of the best tests for efficiency in the modern state. The mineral wealth, the climate, the agricultural resources of a new territory are to be reported upon with a view to its incorporation or development: the men to do this effectively must be ready trained and at hand. A troublesome native tribe is to be tutored by the touch of the masterhand: the man who can guide them with experience, with knowlege of their language, of their religion and customs, cannot be reared‚Äìhe must be forthcoming on the spot.‚Äù (19)\n‚ÄúNo nation can nowadays risk being a single step behindhand in its offensive or defensive services, in its methods of production, of its trade or of transit, or in the general education of its citizens,‚Äìtheir craftsmanship and their ingenuity,‚Äìor, again, in their average physique and reproductive power‚Äù (20)\nThe shift to the importance of intelligence is explained as a change caused by the machine: ‚Äúto-day it is the machine that does the work, and not the man; the important things are the brain which organizes and the intelligence which creates and guides the machine.‚Äù (20)\n\neducation:\n\n‚ÄúSooner or later the primary school must fall absolutely into the hands of the state, and, free from direct local control, be managed by a single council of education and a minster responsible to the national assembly. Every other system is merely tinkering at best; there are not sufficient real educational experts in the country to provide the capacity which is needful on innumerable school boards, to say nothing of parish committees and district councils. Local vigilance committees may well be organizes to see that the national system is effectively carried out locally, but local bodies are not in the intellectual position to draft an efficient educational system; nor, if they could do so, are they able either to put it into practice economically, or to avoid the friction of local sectarian feeling.‚Äù (21-22)\nschools as surveying institutions: ‚ÄúBut state control of primary schools is not only essential from this aspect, but also from the importance which must be attached to the nation having a complete and uniform record of the physical condition of its children. Is the stamina of the nation being not only maintained but strengthened?‚Äù (22)\n\n‚Äúa systematic anthropometric record of the schools would tell us whether our children progress or not from generation to generation, and what is the nature of the special precautions, if any, to be taken with regard not only to individuals but to whole localities.‚Äù (23)\n‚Äúthe sorting and sifting of population, the creation of a local sub-race, suitable to a developing local industry, is by no means so rapid as it ought to be. An effective record, made on a common system, of the physique and intelligence of the children of the nation would immensely assist the quest ofr suitable types of manual labor or of speical intelligence.‚Äù (23)\n\nschools do too many sports, which takes time away from intelligence acquisition, but sports are also good in other ways (24)\n\nif you‚Äôre going to let the kids do sports, at least measure it so that you can use it in your system of bio-political surveillance (24)\nbut sports are good actually because they‚Äôre correlated with good health\n‚Äúclearly games and aptitude for games ought to be encouraged in primary school.‚Äù (26)\n\nprimary school should focus on motor skills needed for craftsmanship and ‚Äúinquiry into things observed expanding as time goes on into a conception of the methods of science‚Äù (27) facts are secondary\n\n‚ÄúLet the child very gradually become conscious of the fact that man is fittest not as individual, but as society.‚Äù (28)\n‚ÄúThe state, as unsectarian, has first to inculcate the social duties: to emphasize the need of developing the physique, the intelligence, and the spirit of co-operative action as essentials of true patriotism.‚Äù (29)\n\nsecondary school: we need specialization (segregation ‚Äúaccording to the nature of the work they are to undertake in life‚Äù)\n\nthere should be ‚Äúcraft-schools‚Äù all across the country, ‚Äúsubject to a much greater local influence than the primary schools‚Äù - craft schools should not be taught by intellectuals or academics, but by practitioners. All that is needed is the communication of the method of labor. rural craft schools:\n\nmust ‚Äúlay the basis (1) of good craftsmanship and (2) of good citizenship. Under the first heading no form of labor is to be considered beneath educational treatment [(empirical and theoretical elaboration)]‚Äù (30)\n‚ÄúEducation is in no case to leave the feeling that it is finer to follow one trade than another, but is to develop the consciousness that it is a disgrace to follow any craft without intelligent appreciation of the why of its processes.‚Äù\ncraft schools should do apprenticeships (the students should be working) - ‚Äúthe secondary craft-school must inspire its pupils with a desire to know the reason for the rote which apprenticeship is sure to thrust upon them.‚Äù\n\nurban craft schools\n\n‚Äúthese places and their courses are largely chaotic at present. They have not settled whether it is their function to give secondary craft-education to boys and girls, to give higher craft-education to the non-commissioned officers of industry, to train the commissioned officers themselves‚Äìthe proper work of higher technical colleges.‚Äìor to provide cheaply a one-sided and, in nine cases out of ten, inferior academic education for young men and women who believe their success and standing in life will be assured if they are hall-marked with a university degree.‚Äù (34 - honestly go off)\n\nThere should be athleticsc in secondary school (37) ‚Äúbut a certain portion of the time devoted to athletic exercises should now be applied to developing qualities which may hereafter be of service for national offence or defence.‚Äù\nschools that do academic training should be separate from those that do technical training, even at the level of staff (teachers at the technical schools should have gone to them).\n\nanother reiteration of segregation: ‚ÄúThe secondary craft-school, the higher craft-school, the technical college, and the university serve quite different functions, educate for different carers and occupations in life; if economy or convenience bring any two under one roof, then there should be a differentiation of teachers; if even this is not possible, there should at the very least be a differentiation of material and of plan of instruction.‚Äù (35)\n‚ÄúHence it should be a sine qu√¢ non of every craft-school, whether secondary or higher, that each pupil should study one brach of pure science, or one literature, or one historical period, apart from his technical studies, as a field for rational enjoyment in adult life.‚Äù (36)\n‚ÄúIn the case of girls the horizon must appear somewhat narrower, and it is, perhaps, only their teachers and elders who can realize the national importance of those forms of physical training which may aid them to be the healthy mothers of a strong race. Still it is highly important that they should realize they belong to a larger whole: that they have a function in the state as well as a relation to individuals. Bandaging, first-aid, the elements of nursing, the car of infants‚Äìand the aged‚Äìmay all be taught as extensions of household economy, and the social value of such work inculcated‚Äù (38)\nvery strong desire to model the system off of the german education system (42-3)\nthe goal of secondary education ‚Äúwill not be to give the lad information useful to him in his future calling, but to develop his intelligence by the application of scientific method and processes with which he will later be concerned.‚Äù\n‚ÄúAn important addition, however, should be made to teaching of such modern secondary schools, not as part of the recreative but as part of the bread studies,‚Äì a reading knowledge of one, and a speaking knowledge of a second language should be insisted upon.‚Äù (44)\n\n‚ÄúIn any specialize branch of science there are rarely at any given epoch more than two or three master-minds, and these are diverse in country and in tongue. To follow these personally or in written word is an impossibility without linguistic knowledge, and science-abstracts and text-books are a deadening and nigh worthless substitute for direct contact with a master-mind.‚Äù (44)\n\nrunning this modern system in tandem with the old system might be a good way to further differentiate individuals in society (47)\nmuch more elaboration about the need for specialization (and the recreative and also religious studies) (48-49)\n‚ÄúWe must base national education on the need for national reaction against a changing environment; we must consciously prepare for the struggle, and by an intelligent study of human evolution arouse the patriotism and race pride of the young to assist directly in developing their intelligence for national ends.‚Äù (49 - would be good to use for a summary)\ntechincal schools and polytechnics should be organized in the most efficient and non-overlapping way possible (51)\neveryone should get a degree if they want one, even the lower class savages: ‚ÄúWe send peripatetic teachers out to fulfil the all-important function of raising the general culture of the people: we fancy it academic extension, and demand that it shall lead to a university degree. Nay, a degree having come to be looked upon as a mark of caste or gentility, the branding-iron is, in the true democratic state, to be brought to every man‚Äôs chamber‚Äù (51-52)\n\nbut only for show: ‚ÄúNor can we bring science and learning in their highest expression to each student‚Äôs door. He must go out on his go out on his Wanderjahre in pursuit of the master-teacher, or of the school which has specialized in his chosen study. What is true of the university is equally true of the higher craft-school: the student must seek the specialized teacher and the specialized school, and not trust a local polytechnic to be an effective educational omnium gatherum.‚Äù (52)\n\npost secondary education: ‚ÄúWe now turn to the highest forms of education, which, whatever we may hope for in a distant future, can at present only be organized for the brain-workers of the community‚Äìfor its thinkers and leaders.‚Äù (53) - Once again having everything mingling together too much is the downfall of the system because it cannot produce sufficiently specialized teachers to instruct the students (who are there to become specialists)\n\nWe have started again on the wrong system‚Äìmultiplication of little centres, doing their individual best no doubt, but not what is best for the nation. Three or four technical universities would suffice for the whole nation, but we have established fifteen or twenty technical colleges, on the theory that knowledge, like milk, must be delivered at each man‚Äôs door. The result is that all the schools are, broadly speaking, doing the same elementary work, and there is no specialization. (56)\n\n\nthe university: ‚Äúpure science or pure scholarship, without regard to the needs of special industries in profession‚Äù (57)\n\n‚Äúas soon as it becomes a recognized principle that intelligence can be trained and developed by observation, and reasoning on observation applied to technical or professional subjects, much of the monopoly value of pure academic studies will disappear‚Äù (57)\nmostly ‚Äútraining for specialized careers, namely, for statesmen, scientists, historians, literary men, educators, and makers of all forms of knowledge,‚Äìin sort, for the intellectual leaders of the nation‚Äù (58)\n‚Äúacademic studies will become more intense and more definite in character. Above all, the research training will more and more supplant the examination training.‚Äù (58)\n‚Äúwe need a training in method, and not, in the first place, a mere knowledge of facts, nor even the laws under which these facts may be classified. It is so easy to provide facts and formulae, so difficult to give insight into method, that text-books, degree schedules, and examinations invariably turn to the former; and the latter, to be learn only from direct touch with the investigator or from the classical memoir of the master, is thrust ruthlessly aside.‚Äù (59)\n‚ÄúThe university of the future will bring its undergraduates, not into touch with an army of tutors and‚Äùcoaches,‚Äù nor with their impedimenta of examination schedules and text-books, but directly into the field, the library, the laboratory, where the material of knowledge is accumulated and classified, and into personal touch with the men who make it.‚Äù (60)\n\nthe technical college: technical colleges should become specialized technical universities (56), of which there will be only a small need. The others should be closed or converted to higher craft-schools.\n\n‚ÄúUse their staff or buildings, where possible, for special departments of the university, but recognize once and for all that under the stress of modern competition these are matters of national importance; and that to bring our technical intelligence up to the level of that of our neighbors, we do not want local engineering professors, or local colleges, but national technical universities, each with ten or more complete laboratories, a score of special technical professors, and with equipment and funds comparable only with those of the whole of pure-science faculty of a first-class modern university. Such universities would train not only the nation‚Äôs industrial leaders but the teachers for the secondary and higher craft-schools; and by bringing both classes into touch with actual knowledge-making, indicate on the one hand how the problems of practical life, on the other the problems of craft education, may be met and solved.‚Äù (57)\n\nthe professional schools:\n\nlaw school: comparative and historical knowledge is lacking (62), leading to low imagination in writing legislation and ‚Äútoo little sympathy in dealing with legal institutions of subject or assimilated races.‚Äù\nmedical school: ‚Äúit is impossible that the current system of support by fluctuating charity can permanently continue. Alongside the public charities have arisen infirmaries, fever hospitals, and asylums supported by public funds, and in many cases but little used for clinical instruction.‚Äù (62)\n\nneed for scientific method in medicine (65): ‚ÄúMuch of the strength of proof in medical science depends entirely on statistics; copious raw material can be obtained from hospital practice, but this is rather too largely drawn from special classes of the community. the bulk of data from all classes either escapes written record, or remained‚Äùunstandardized‚Äù in case-books; here it is monopolized by the individual as ‚Äúexperience,‚Äù when by co-operative action it might be statistically generalized into proof. The quantitative value of the correlation between environment, age, or physical characters and the special features or virulence of any disease is probably unknown at the present day in a single instance; and yet it is hard to conceive that clinical prognosis would not be greatly advanced, especially among the younger members of the profession, by a quantitative knowledge of this kind. An authoritative body standardizing records, collecting individual experience and reducing it by adequate statistical theory, seems almost a necessity for medical progress at the present day.‚Äù (66)\n\n\nthe commercial university: for commerce and business studies; these people will be doing fiscal policy and negotiating terrifs (67).\n\nthese guys seem to be really involved in the colonialism part of this project; the instructors are to take one out of every 3 years travelling (67), as in the Russian system\n‚ÄúTo the commercial university a relatively considerable number of studentships should be attached, the holders of which should be compelled to travel and report on foreign and colonial commercial methods and possibilities. These reports should in the first place be looked upon as exercises, but selected reports might well deserve publication as monographs and commercial research. Past holders of such scholarships, the pick of academic training; with their minds freed from insular method and local custom by the insight of travel, would undoubtedly be in constant demand for pioneer work.‚Äù (69)\n‚Äúthey could yearly send out men to study the flora and fauna of almost untouched districts; to learn the native languages, religions, and customary laws of British and other possessions; to study under the masters of pure science, history, or philosophy who exist outside their own walls; and to return, as the American travelling fellows have done from their European universities, to develop their home institutions and widen their educational systems by leaps and bounds.‚Äù (70)\n\n\ngovernment schools: ‚Äúit is very desirable that the government schools should be limited to those branches of instruction which are needed only for the national service. For example, schools of offence and defence‚Äìnaval colleges, staff colleges, artillery and military engineering schools; to these ought probably to be added, schools for home and imperial civil service‚Äìfor consuls, native state residents, and the lower branches of the diplomatic service‚Äù (71)\n\ngospel of specialization again\nefficiency - only put government schools where they will not overlap with adequate offerings from another educational body (73)\n\n‚ÄúOne of the greatest dangers of science, and especially science in the consultative science of the state, is the possible creation of a scientific hierarchy, resting on past achievement and believing itself at the summit of scientific knowledge. As soon as man ceases to research, he has fallen behindhand; his tools grow rusty, and he ceases to grasp new methods and possibilities. Hence one of the greatest problems of the state is how to draw into its service not only those who have achieve as consultants, but those who are achieving as discoverers.\n\nprizes! (with monetary rewards) ‚Äúsuch prizes and research should be independent of national laboratories and government consultants, to whom more specialized problems and routine difficulties should be submitted for solution or advice.‚Äù (76)\n‚ÄúHere, as in other fields, a differentiation of pure and applied sciences is necessary.‚Äù (87)\n\nmany many more examples of technical schools, which I suppose the government is going to sponsor or run (? not super clear about those details)\n\nabout astronomy, geography, geology, and meteorology departments: ‚ÄúThe link between the central home observatories and those in the colonies and dependencies has hardly been strong enough, nor the whole chain of institutes systematized; and this is particularly the case in the meteorological service‚Äù (81)\n\n\nMedical and Sanitary Institutes\n\n‚ÄúHere again therea re innumerable questions to which municipalities, or home and colonial governments, need answers, and definite and prompt answers‚Äù (83)\nthe institutes of medicine and sanitation should preempt the public call for health and sanitation services because by that point it will be too late (see covid)\n\nnational institute for anthropology\n\n‚ÄúWith possibly more races under the British flag than under any other imperial symbol since the days of the Roman eagle, we have yet entirely failed to systematize and nationalize our study of those races. There is national museum or institute where one may learn the cranial, anthropometric, and physical characters of the various races under our sway, still less something of their languages, folk-customs, industries, and religions. An institute carrying out a complete anthropological survey of the empire is as necessary from the imperial standpoint as those dealing with geographical or geological surveys. The Americans have recognized this, and their anthropological reports and museums under state supervision will soon be a model for such work everywhere.‚Äù (84)\n\nbotany and zoology - zoology apparently behind that of the Americans (87)\nthe biological farm (87) - a special farm that serves doubly as a biology laboratory\n\n‚Äúsuch statistical and secular experiments cannot be stisfactorily undertaken in existing biological laboratories: they need a considerable range and variety of lang, water, wood, and open field, free from intrusion and under the management of skilled keepers; there must be shed for the breeding of, at any rate, the smaller mammals, houses for insects and birds, and ample space for all kinds of other extensive experiments on heredity and variation. There must be, in addition, workroom, microscope rooms, dissecting laboratories, and instrument departments for the use of measures, computers, and researchers. Such a biological farm could in the present state of affairs do epoch-making work within the space of a very few years.‚Äù (88)\n‚ÄúIt ought to be perfectly possible in a few years to determine, to the satisfaction of all parties concerned, the limits of truth in the laws propounded by Galton or Mendel, or any other; and only by such unbiased experiments, not by controversial publications, can the actual facts be reached. It is admitted on all sides that we stand here, in the matter of variation and heredity, on the eve of wide-reaching discoveries, and only an institute such as we have sketched can really conduct the extensive and secular experiments which are needed for truly authoritative answers‚Äù (88-9)\n\n\n‚ÄúThe crowning study of man is man; the highest science is that which deals with human races, and sees the causes which lead to their progression and relative dominance. This science, applied to national life, is statecraft,‚Äìthe art of seeing what makes for national health and for national fitness. Every nation, however, is an agglomeration of classes and castes, of the mentally and physically healthy, and of the mentally and physically unsound. Man, if the highest of living forms, is still one of them; and it is easy to test whether the general laws of heredity and relative fertility provided for the lower forms hold, with or without modification, for him. Problems as to the reproductive dominance of better stocks, as to effective national fertility, as to progressive physical and mental development in man, will be vital problems for the statesmen of the near future; their solution is closely bound up with a knowledge of the laws of heredity, fertility, and variation in other living forms, which only such an institute as we have just described can effectively study.‚Äù (89-90)\n\na call for science, particularly specialized journals, to be funded more, and for more specialized journals to be created\nThe state should have science counselors and consultants (95)\n\nall about fitting the right man to the right role (96)\n\n\nWe have by one of other process to learn the national importance of science: to realize that science in the broadest sense, as an educator and discoverer, is the mainspring of modern national life; that the future is to the scientifically trained nation which reproduces itself, maintains its health, develops its institutions, controls its production, organizes its distribution, extends its territory, governs its subject races, and prepares its offensive and defensive services with scientific foresight and insight‚Äì\n\nIn the reproof of chance\nLies the true proof of men‚Äî\n\nand, we may add, the true proof of nations (97)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#letters-to-the-editor",
    "href": "06_eugenics.html#letters-to-the-editor",
    "title": "6¬† Regression and Power",
    "section": "7.5 Letters to the Editor",
    "text": "7.5 Letters to the Editor\n(CuttingsContainingLetters1905?)\n\nPearson: ‚ÄúI feel very strongly that this matter is of far greater public importance than any personal controversy between Dr.¬†Donkin and myself. A knowledge of the heredity or non-heredity of character of the tendency to commit anti-social acts must be antecedent to any profitable scheme of criminal reform.‚Äù\n‚ÄúWe have at moment a new and active Home Secretary. May I suggest that one of the most valuable additions that could possible be made to the Prison Department at the present time would be the appointment of a medical man with one or two assistants, whose special occupation should be tracing the family history (chiefly from police records) and environmental conditions in early life of convicted criminals? We should soon have sufficient material on which a definite judgement might be based as to whether crime or the tendency to law-breaking is or is not hereditary.‚Äù\nelder born children are better and live longer somehow (and they may have more ability)\n‚ÄúThe inheritance of ability is so marked that there is every reason to suppose that a man who has won his way to pure ability to the House of Lords will, if he has mated wisely, have children above average in ability. Unfortunately, the House of Lords has too often been recruited by mere plutocrats, by political failures, or by men who have not taken the pains necessary to found or preserve an able stock.‚Äù\nGalton seeks to establish that the natural ability in England springs forth from just a few great families by analyzing 200 survey responses from the Royal Society. In the analysis, he shows some attention to factors other than mere inheritance (i.e., also paying heed to nurture)",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#practical-eugenics",
    "href": "06_eugenics.html#practical-eugenics",
    "title": "6¬† Regression and Power",
    "section": "7.6 Practical Eugenics",
    "text": "7.6 Practical Eugenics\nPearson (1909) - a laypersons introduction to the science of Eugenics\n\n‚ÄúWhen we apply this method to various characteristics in man, we find that the degree of resemblance between parent and offspring lies between .4 and .5; betweek offspring and grandparent between .2 and .3; and between any grade of ancestor and the offspring the resemblance diminished in geometrical progression, the factor of reduction lying between .5 and .6‚Äù (4)\n\nthis is a natural law present in humans as in ‚Äúhorses, dogs, and cattle‚Äù\nyou cannot explain albinism, as Pearson documented it, using Mendelian inheritance\nthe characteristics explained by Mendelian genetics are not complex or socially meaningful enough to be useful in a regime of population control, even for the comparatively minor task of selectively breeding some traits into horses\n‚ÄúYou cannot profitably group men into tall and short, into pure blue eyed and non-blue eyed, into albinos and all the rest. The finer grading and statistical observation of how much quantitative differences in the parents or ancestry influence the grade of the off-spring are essential to our judgement of hereditary influence in man.‚Äù (5) hm\nDoesn‚Äôt this proposed natural law correspond almost exactly to the predictions made by Mendelian genetics? NO! it wouldn‚Äôt because of the dominant/recessive thing?\n\n‚ÄúGood home environment was shown to have practically no influence on the intelligence of boys, on girls it was represented by a correlation of .07, hardly 1/7 the intensity of heredity. The relationship between eyesight and home condition was practically insensible, and the effects of employment of mother on the physique of the children, or of the drinking of the parents on the intelligence of the children were practically of no importance compared with the fundamental factor of heredity.‚Äù (6)\n\n‚ÄúHave not the numbers given in the past lectures taught us then a first fundamental principle of practical Eugenics? It is five to ten times as advantageous to improve the condition of the race through parentage as through change of environment.‚Äù (6)\noutlawing child labor is bad actually - because evolution\n\n‚ÄúFormerly a child became at an early age a pecuniary asset. It contributed to the family maintenance by six or eight years of age, and by the number of children the economic prosperity of the home was in a certain sense measured. That a child should be looked upon as a ‚Äòpecuniary asset‚Äô shocks many of us, as it shocked Lord Shaftsbury. But from inquiries I have made, the condition of the child as a ‚Äòpecuniary asset‚Äô was not wholly a bad one; it must be kept in health, because it ceased to have any pecuniary value if it broke down. A Bradford doctor assured me that in the days before the factory acts more care was taken of the children on this very account. No Bradford woman of those days would have replied as to the number of her children:‚ÄùSixteen, but thank the Lord, thirteen of them are in the churchyard.‚Äù The effect of lead-poisoning and the professional abortionist were then practically unknown in the manufacturing centres.\n‚ÄúThe mistake of most legislation is that it is carried by appeal to the sentiment and feelings of relatively small classes‚Äìthe cultured and highly sensitive upper and middle class. The biological and economic bases of life are disregarded, and the result is only manifest some twenty or thirty years later.‚Äù (7-8)\n\n\nanother piece of factory legislation: restrictions on pregnant mothers working: ‚ÄúWe picture the child and the mother toiling in the factory, and we, judging the matter from our own feelings and cultured sentiment, shudder and‚Äìturn them out. We never regard the matter from the economic standpoint, and do not realise that in our well meant action we have taken a great step towards the abolition of both children and motherhood.‚Äù (8)\nallegedly progressive factory legislation removes the possibility of the child being a pecuniary asset until it has reached working age (which was inflated up to 13-15 years)\n\nBy force of economy, this discouraged motherhood\n\nbirthrate computation (10)\n\nthe birthrate is falling (plates 1 and 2, p.¬†13 and 15). why?\n\nis the change in the distribution of the ages of mothers causing the drop in births (this is an effect of mothers living longer, on average, due to the medical innovations)?\n\n‚ÄúI believe this to be largely the source of the fall in birthrate in our Colonies, for only the active younger women emigrated in the early days. Now that emigration is not the chief source of the population, there are many more elderly women, and the birthrate has naturally fallen.‚Äù (10)\nPlates 1 and 2 are the fruits of the endeavor to answer this question. They show (in Cornwall and Bradford) that the birth rate curve for just married women 15-55 has the same slope as the curve for all married women (‚Äúall possibly reproductive wives‚Äù, 11), only it is higher (to account for the fewer births among older women)\n\nwhen did the fall in birth-rates start? Now we are going to look for spurious correlations between factory legislation and birthrates, if I had to guess\n\n‚Äúslight effect in 1867, marked effect in 1877, very marked effect in 1887 and accelerative effect in or about 1892.\nNorfolk, rural district: ‚ÄúWives become more numerous, but mothers fewer‚Äù (12) in like 1887\nNorth Riding of Yorkshire & Middleborough, rural district?: the birthrate decreases more substantially since 1887 when you exclude the part of town where there a lot of young women moved and the birth rate drastically increased.\nYork (plate 2), town with county and trading occupations but ‚Äúno strong manufacturing interest‚Äù (13): reduction since 1887\nmanchester, manufacturing town with other trading interests: decrease\nBradford (plate 2), manufacturing town: there was an extraordinary decrease starting in 1877, and now ‚Äúthe population of Bradford without immigration would hardly be maintaining itself at the present rate‚Äù (13) given an assumption of 30 in 100 infant mortality\nHuddersfield (plate 2), manufacturing town: marked decrease started in 1877, as with Bradford\nBolton (plate 3), textiles manufacturing town: decrease began in 1877 and accelerated in 1892\nLeeds (plate 3), engineering and textile centre: decrease begins in 1877 and accelerates from 1892 onwards\n‚ÄúNow I think it impossible to study such curves as I have put before you and not appreciate the national gravity of the situation. The English population has not yet reached, but is in a fair way to reach in the course of the next fifteen years, the condition of France in which it will not reproduce itself and will depend for maintenance on immigration‚Äù (14)\n\nnot warning the brits they could wind up like the French, who are among the neighbors we are preparing for in war via our education in Pearson (1919) (? right citation)\n\n\ntrends in vital statistics (birthrate) lead to the following questions which seek to clarify the cause via a series of spurious correlations (I think this is it!!):\n\nwhat happened in 1867 that may have explained the decrease in Huddersfield?\n\nWorkshop Regulation Act of 1867: ‚ÄúBy this act no child under eight was to be employed in any handicraft, children from 8 to 13 only to be employed half-time. I believe this to be actually the most important step taken up to that date to destroy the economic value of the child.‚Äù (15) - even though the effect was only in one town in 1867? maybe it took more than a decade to arrive elsewhere?\n\nWhat happened in 1877 that affected the textile and engineering tows?\n\n1874: factory law raises minimum working age to 10\n1878: being super fucking dramatic: ‚ÄúThis act of 1878 was extremely complex and calculated on this very ground to discourage the employment of children. Children defined to be persons under 14 were to be employed for half time only‚Äìin morning or afternoon sets or on alternate days. A child must not be employed for two successive periods of seven days in the same set, whether morning or afternoon, nor on two successive Saturdays, nor on Saturday in any week, if he has already in that week been employed on one day more than five and a half hours. Nor shall a child be employed fully on two successive days, nor on the same day in two successive weeks. Employment of the children at home when work is the same as in the factory or workshop was also regulated. No child under ten was to be employed, and medical certificates were required in the case of all children and young persons under 16‚Äù (15)\n\nmuch more succinct: ‚Äúmost effectively tended to destroy the child as an ‚Äòeconomic asset.‚Äô Children under ten could not be employed at all, children under 14 could only be employed half time and this in a complicated way. All persons under 16 required a medical certificate‚Äù (16)\n\n\nwhat happened in 1887 which had a general effect across towns, including ‚Äúcertain non-urban districts and trading towns‚Äù?\n\nMines Act in 1887: prohibited employment of boys under age 12 underground, and the employment of boys and girls above ground in mining/mineral preparation work. ‚ÄúThis act directly touched interests in Cornwall and North Yorkshire‚Äù (16)\n1891: age of employment raised again, and employment of women immediately after childbirth (16)\n1899: ‚Äúthe Education Act made it unlawful to emply any child under 12 in such a manner as to prevent full attendance at school‚Äù\n‚Äú, and in 1901 the prohibition of the employment of any child under 12 in factory or workshop was made direct and absolute‚Äù (16)\n\nuniversal education reforms were also important in minimizing the economic value of the child\n‚ÄúIf, as I believe, our present precarious condition with regard to the birthrate is a direct effect of the destruction by legislation of the economic value of the child, surely a great lesson may be drawn for practical eugenics? Does it not demonstrate that whatever law affects the economic status of a portion of a community must also be dealt with from its biological aspects?‚Äù (!8)\n\n\nother effects of a lower birthrate\n\nfirst and second born children are ‚Äúof a more nervous and less stable constitution. We find the neurotic, the insane, the tuberculous, and the albinotic are frequent among the elder-born. Dr Goring‚Äôs results for criminality show the same law. The diagram (see plate 4) I put before you will bring this outl; you see in the tuberculous, the insane and criminal stocks that the first few members are weighted.‚Äù (19)\n‚ÄúThe result of this law is remarkable. It means that if you reduce the size of the family you will tend to decrease the relative proportion of the mentally and physically sound in the community.‚Äù (19)\n\n\nNow it seems to me that we have an illustration in this matter of a case‚Äìand it is not an isolated case‚Äìin which legislation intended to promote national progress‚Äìto improve the radical qualities of future generations‚Äìhas directly tended to enfeeble the race. (19)\n\n\nam I advocating that we should repeal the factory laws? ‚ÄúAssuredly not. I wish, however to emphasize two practical points. The first is, legislation intended to increase racial fitness may end by penalising parentage and motherhood. The second is, that the economic value of the child will in the long run govern its production. All legislation which places parents in an economically worse position than the unmarried is radically unsound.‚Äù (20)\n\nyou can solve the problem of practical eugenics by comodifying children and then regulating them like a comodity\n\n‚ÄúThe child is economically a commodity and like any other ware is produced to meet the demand; for the great bulk of the population whose wages extend but little beyond subsistence, the child will be produced or not according as it has economic value. If we can give the child economic value, the birthrate will rise; if we can differentiate between good and bad parentage, if we can make the possession of healthy, sound children a greater economic asset than the possession of feeble offspring, then we have for the mass of people solved the problem of practical eugenics‚Äù (21)\ndid he know that this was immoral at the time. Yes! ‚ÄúI am very fully aware that this fundamental principle that the child is a ware and, in a community which has learnt how to restrict its birth rate, will be produced in proportion to its economic value, will not be a popular doctrine.‚Äù (21)\n\nlegitimately subversive take: of course the rich sentimental people won‚Äôt understand this, ‚Äúwith these classes the child has never been an economic asset; it is a luxury which we know we must pay for, and expect to pay for, until after college and professional training, and, in the case of unmarried daughters, often long after our own lives are concluded‚Äù (21)\nthis upper class logic has no business being extended to lower classes to which it does not apply\n‚ÄúWe can, in the case of these cultured classes, urge great social principles, and ultimately create social sanctions, for the parentage of the fitter and the sterility of the unfitter stocks. This is a moral crusade, and I believe it will be successful, however many are the prejudices and difficulties it will have to encounter.‚Äù (21-2)\n\n\ntwo fundamental problems for practical eugenics:\n\nproducing ‚Äúa sufficient supply of leaders of ability and energy for the community‚Äù\n\nyou can use ‚Äúmoral teaching backed by social sanction‚Äù for these folks, but not for the workers who are living on subsistence wages\n\nproviding ‚Äúintelligent and healthy men and women for the great army of workers‚Äù\n\nthese people follow the rules of the capitalist market\n‚ÄúThere is, I believe, one way, and one way only, of solving this problem: we must reverse the effect of the factory acts which have penalized parentage and handicapped motherhood. Both the reversal must be done in a differential manner, sound parentage and healthy motherhood must be given a substantial economic advantage over not only childlessness, but over unsound parentage and feeble motherhood; the well-born child must again be made a valuable economic asset. This is the central problem of all practical eugenics,‚Äìeugenics as a doctrine of national welfare is a branch of national economy.‚Äù (23)\nwhere is the moral panic? ‚ÄúBefore touching possible directions of reform, I want to point out to you that while a penalisation of parentage is bad,‚Äìfor, given the material, Nature, the first and most thorough practical eugenicist, will play her part in selection‚Äìyet our special penalisation is excessively bad; for it has, owing to municipal and charitable institutions, emphasizes the penalty in the case of the better type of parent.‚Äù (23)\n‚ÄúThe thrifty, provident parents who wish to provide a home life for their offspring not only find themselves penalised as against their childless competitors,but as against the thriftless and improvident who throw the burden of their children on public rates and on private charities. I want to bring this out emphaticaly because it seems to me an essential part of practical eugenic policy to protect and fight against this municipal and charitable method of penalising better parentage.‚Äù (25)\nTable 1 shows that the birthrate for various ‚Äúpathological‚Äù type folks is higher than for ‚Äúnormal‚Äù folks in most cases (and in some cases much higher)\n\nbecause these people are provided for largely by the government, their children do not come with the same economic disadvantage that the normal people are subject to (those being the normal workers who are subject to the child as commodity system; the intellectual elite have the lowest birthrates and are subject to different calculus).\nTable 2 gives a lot of scary-looking correlations\n\n‚ÄúThis table shows in every case (cancer is the only exception we know) a positive correlation between an undesirable social feature and a high birthrate, and a negative correlation between a mark of well-to-do population and the birthrate.‚Äù (26)\n\nthe correlations are getting worse (shown in table 3) - I don‚Äôt get them\n\n\n\n\nAll separate lines of inquiry tend to confirm the view that the districts of a good social character have the lowest birthrates; that the anti-social stocks are at present most prolific, and this whether we measure the gross or net fertility.\n\n\n‚Äúthe child ceasing to be an economic asset, has become a burden, but poor law and charity have largely succeeded in lifting this burden from the shoulders of the degenerate parents. We have not only hindered Nature from weeding out social wastage, but we have made the conditions increasingly more favourable to the multiplication of this degeneracy. Practical eugenists must urgently demand the reversal of all legislation which penalises the parentage of the fit, and the restriction of all charity which favors the parentage of the unfit. we must directly or indirectly produce differential wage for the fit parent; in other words there must be endowment of fit parentage at the expense of the unfit parent and of childless men and women.‚Äù (29)\nhow to do it?\n\ndifferential taxation (Lloyd George‚Äôs proposal will be ineffective - georgism, if I recall?): ‚ÄúIt will be the fault of eugenic workers if the thin end of the wedge thus inserted be not driven home. Taxation must differentiate between the parent and the non-parent in income-tax, settled estate duty and death duties.‚Äù (29)\nother economic means (30), modeled after Germany and the Indian Civil Service: ‚ÄúAt first a very rough standard of differentiation would suffice‚Äìa fairly clean bill of health for both parents, and absence of obvious taint in their immediate stock, a moderate school standard passed, and a minimum wage value in the market to test general ability. Even without this slight test‚Äìwhich at any rate would exclude the epileptic, the deformed, the insane, and the deaf-mute stocks from the benefits of the scheme‚Äìwe should by a simple insurance fund of this kind have removed the present disabilities of parentage which, as I have endeavoured to show, are practically differential with regard to the fitter parentage.‚Äù (31)\n‚ÄúWe see enormous sums annually given for charitable purposes without the least attempt to differentiate between the recipients who spring from fit and those who spring from unfit parentages, between the recipients who are of racial value and those who are mere social wastage. Asylums abound for the imbecile and the cripple, homes for the waifs and strays, orphanages, hospitals, the boast of which is that they receive without selection all sufferers. Do the subsrcribers to these and many other kindred institutions ever consider that they are directly penalising fit parentage by enabling the unfit parent to obtain provision for his deformed of diseased offspring?‚Äù (31-32)\n\n‚ÄúIs it not possible by aid of a little educative propagandism of a eugenic character to divert some of the thousands we see every week willed to indiscriminate charity in a more rational and national channel? Why should they not be ear-marked in some small percentage of cases for the offspring of fit parentage? Cecil Rhodes with the insight of a strong man determined that Rhodes scholars should be selected for ability, physique, and character combined. Such a combination will rarely be found without fit parentage, and probably the best means of securing it would be a study of ancestry.‚Äù (32)\n\n‚ÄúThe whole system of secondary school and university scholarships provided by the educational committees of the County Councils wants at present stable basis. The candidates too often lack the physique and character, without which mere examination ability is worthless. Here again is a wide field for eugenic effort, for the indirect endowment of the fitter parentage.‚Äù (33)\n‚ÄúI am very fully conscious that there are many other direction than those I have advocated to-day wherein the eugenist can work towards racial improvement. But I have chosen the two points‚Äìfactory legislation and modern charity‚Äìbecause I believe they are the sources of our gravest present difficulties. Both of them mark the extreme limit of philanthropic effort‚Äìthe attempt to improve the racial fitness of the nation by purely environmental reforms, the removal of the child and mother from unhealthy surroundings, and the provision for the weak and the suffering. Both have failed in promoting racial efficiency, because they overlooked great all-mastering biological laws. after 60 years of philanthropic effort unparalleled in any European country, we find ourselves as a race confronted with race suicide; we watch with concern the loss of our formal racial stability and national stamina.\n\n‚Äúan artificial birthrate has been created in the fitter classes, which may become habitual, and if so spells ultimate racial destruction.‚Äù (35)\n‚ÄúThis view of human society which has been given in this lecture, will I fear prove unpopular‚Äìthat is not my mind in an argument against its truth. I would not ask you to accept it without much criticism, and without viewing it from every possible side. To some of you who do this it will become a real possession, which will unify your conceptions of our present difficulties as to the apparent incompatibility of the highest forms of civilisation with continuous race progress. Why do we find degeneracy and race suicide arise as human sympathies and emotions are widened? I think the answer lies in the fact that environment appeals directly to our senses, but heredity only to our reasoning. We rush to modify the former, regardless of the laws of the latter. The releif of pain and suffering is so obvious a duty, the penalisation of suffering is so obvious a duty, the penalisation of parentage is so disguised and so distant in its effects. When we say: ‚ÄòYou must protect the child from unhealthy or cruel environment‚Äô the best of the nation is with us with vote and even with purse. When we say ‚ÄòYou must preserve the economic value of the child,‚Äô we evoke no sympathy; none see at once the whole tale of penalised parentage, lowered birthrate, cacogenic reproduction, race degeneracy and the ultimate race suicide involved in the breach of that principle.‚Äù (36)\n\n‚ÄúThe child as comodity whose supply is regulated by economic value may sound a harsh doctrine. But truth‚Äìwhether of natural selection or of social evolution‚Äìis not created by man; he has only to discover it, be palatable or bitter. Social stability depends upon the extent to which we allow even unpalatable truth to guide our legislation and our conduct. Eugenists have before them at present alternative paths, they can follow the easy course of appeal to popular feeling and untutored human emotion, in which case they will create, like philanthropic effort, immediate interest, have their day and their fashions, and leave no progressive impress on racial evolution. Or, they can take the harder road of first ascertaining the laws which regulate the human herd, of creating a science which shall dictate an ultimate eugenic art. In the latter case they will scarcely be popular for I feel sure their truths will be bitter and our generation likes above all things its medicine and mild and toothsome form.‚Äù (37-38)\n\n\nm√ºnecat (2024):\n\nvery long video essay about how bullshit evolutionary psychology is - does a very nice job imo; also very good (and funny) examples and presentation\nmodern evolutionary psychology, unlike eugenics, takes a more reasonable view of the timescale of evolution - at least thousands of years, but always heavily dependent and sometimes taking place on the scale of hundreds and thousands of years.\nmodern evo psych is also bullshit, but for a different reason. The psychologists, knowing nothing about what human culture a hundred thousand years ago, just superimpose their own (gendered) impression of the state of nature.\n\n\n\n\n\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nm√ºnecat, dir. 2024. I Debunked Evolutionary Psychology. https://www.youtube.com/watch?v=31e0RcImReY.\n\n\nPearson, Karl. 1909. The Problem of Practical Eugenics / by Karl Pearson. Eugenics Laboratory Lecture Series 5. London: Dulau and Co.\n\n\n‚Äî‚Äî‚Äî. 1919. The Function of Science in the Modern State / by Karl Pearson. 2nd ed. Eugenics Lecture Series 12. Cambridge: University Press.\n\n\n‚Äî‚Äî‚Äî. 1920. The Science of Man: Its Needs and Its Prospects / by Karl Pearson. Questions of the Day and of the Fray ; No. 10. London: Cambridge University Press.\n\n\nPearson, Karl, and Alice Lee. 1903. ‚ÄúOn the Laws of Inheritance in Man: I. Inheritance of Physical Characters.‚Äù Biometrika 2 (4): 357‚Äì462. https://doi.org/10.2307/2331507.",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "06_eugenics.html#footnotes",
    "href": "06_eugenics.html#footnotes",
    "title": "6¬† Regression and Power",
    "section": "",
    "text": "The ‚ÄúRule of Three‚Äù is a method for solving linear systems of equations (i.e., it is a method for determining unknown values in a system of variables).‚Ü©Ô∏é\n‚ÄúIf you tell me that we are here trenching on the field of psychology and medicine, I reply: Certainly; you do not suppose that any form of investigation which deals with man‚Äìbody or mind‚Äìis to be omitted from the science of man? If you do you have failed to grasp why anthropology is the queen of the sciences. The University anthropological institute of the future will have attached to it a psychologist, a medical officer, and a biologist. They are essential portions of its requisite staff, but this is a very different matter from lopping off large and important branches of its fitting studies, to lie neglected on the ground, or to be dragged away, as dead wood, to be hewn and sharpen for other purposed by colleagues in other institutes. Remember that I am emphasising the size of anthropology which studies man in the service of the State‚Äìanthropology as a utile science‚Äìand that this is the only ground on which anthropology can appeal for support and sympathy from State, from municipality, and from private donors.‚Äù (Pearson 1920)‚Ü©Ô∏é",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Regression and Power</span>"
    ]
  },
  {
    "objectID": "0R_references.html",
    "href": "0R_references.html",
    "title": "References",
    "section": "",
    "text": "Bakker, Arthur, and Koeno P. E. Gravemeijer. 2006. ‚ÄúAn\nHistorical Phenomenology of Mean and\nMedian.‚Äù Educational Studies in Mathematics\n62 (2): 149‚Äì68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. ‚ÄúA Brief History of\nS.‚Äù In Computational\nStatistics, edited by Peter Dirschedl and R√ºdiger\nOstermann, 81‚Äì110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. ‚ÄúCase\nStudy 1: AT&T\nDivestiture.‚Äù Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of\nSexuality. Vol. 1. 3 vols. Random House.\n\n\nFoundation, Free Software. n.d. ‚ÄúWhat Is Free\nSoftware? - GNU Project - Free Software\nFoundation.‚Äù Accessed May 9, 2024. https://www.gnu.org/philosophy/free-sw.html.\n\n\nIhaka, Ross, and Robert Gentleman. 1996. ‚ÄúR: A\nLanguage for Data Analysis and\nGraphics.‚Äù Journal of Computational and\nGraphical Statistics 5 (3): 299‚Äì314. https://doi.org/10.2307/1390807.\n\n\nm√ºnecat, dir. 2024. I Debunked Evolutionary\nPsychology. https://www.youtube.com/watch?v=31e0RcImReY.\n\n\nPearson, Karl. 1909. The Problem of Practical Eugenics / by\nKarl Pearson. Eugenics Laboratory Lecture Series 5.\nLondon: Dulau and Co.\n\n\n‚Äî‚Äî‚Äî. 1919. The Function of Science in the Modern State / by\nKarl Pearson. 2nd ed. Eugenics Lecture Series 12.\nCambridge: University Press.\n\n\n‚Äî‚Äî‚Äî. 1920. The Science of Man: Its Needs and Its Prospects / by\nKarl Pearson. Questions of the Day and of the Fray ;\nNo. 10. London: Cambridge University Press.\n\n\nPearson, Karl, and Alice Lee. 1903. ‚ÄúOn the Laws of\nInheritance in Man: I.\nInheritance of Physical Characters.‚Äù\nBiometrika 2 (4): 357‚Äì462. https://doi.org/10.2307/2331507.\n\n\nShustek, Leonard J. 2016. ‚ÄúProgramming the ENIAC:\nAn Example of Why Computer History Is\nHard.‚Äù May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. ‚ÄúThe Medicalization of\n‚ÄòHomosexuality‚Äô.‚Äù Honors Capstone\nProjects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTukey, John W. 1972. ‚ÄúData Analysis, Computation and\nMathematics.‚Äù Quarterly of Applied Mathematics 30 (1):\n51‚Äì65. https://doi.org/10.1090/qam/99740.\n\n\nTuring, Alan. 1936. ‚ÄúOn Computable Numbers, with an Application to\nthe Entscheidungsproblem.‚Äù Journal of Math\n58 (5): 345‚Äì63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.\n\n\nWickham, Hadley, Mine √áetinkaya-Rundel, and Garrett Grolemund. 2023.\n‚ÄúData Visualization.‚Äù In R for Data\nScience, 2nd ed. https://r4ds.hadley.nz/data-visualize.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. ‚ÄúData\nVisualization.‚Äù In R for Data Science, 1st\ned. https://r4ds.had.co.nz/data-visualisation.html.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "0A_resources.html",
    "href": "0A_resources.html",
    "title": "Appendix A ‚Äî Getting Help",
    "section": "",
    "text": "The contents of this appendix are not here!\n\ngetting help in R (? and ??)\nthe two types of help pages, and the structure of each\n\ndataset pages\nfunction pages\n\nCheatsheets\ndocumentation online\n\nrmarkdown\nquarto:\n\nquarto guides\nquarto reference\n\nrstudio user guide\ntidymodels (for machine learning, mostly)\ngoogle and stack exchange\n\nThere is a general guide to getting R help, and it includes a suggestion I will forward: use Google and include the term ‚ÄúR‚Äù in the search",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Getting Help</span>"
    ]
  }
]