[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hunterr_2024",
    "section": "",
    "text": "Preface",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "hunterr_2024",
    "section": "",
    "text": "pre-processing here refers to very basic, non-mathematical data processing tasks. Pre-processing involves developing a principled framework for storing data and then converting data from the wild into that format.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  The Lore",
    "section": "",
    "text": "1.1 Early 19th Century: the birth of programming\nThe idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; and then the program runs.\nFolks in the 19th century did not have access to digital text files, and so they could not write their computers on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this section?",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n\nshow code for this result\n1 + 1\n\n\n[1] 2",
    "crumbs": [
      "Statistics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bakker, Arthur, and Koeno P. E. Gravemeijer. 2006. “An\nHistorical Phenomenology of Mean and\nMedian.” Educational Studies in Mathematics\n62 (2): 149–68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. “A Brief History of\nS.” In Computational\nStatistics, edited by Peter Dirschedl and Rüdiger\nOstermann, 81–110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. “Case\nStudy 1: AT&T\nDivestiture.” Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of\nSexuality. Vol. 1. 3 vols. Random House.\n\n\n“S (Programming Language).” n.d. Wikipedia. Accessed May 6,\n2024. https://en.wikipedia.org/w/index.php?title=S_(programming_language)&oldid=1212530456.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. “The Medicalization of\n‘Homosexuality’.” Honors Capstone\nProjects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTuring, Alan. 1936. “On Computable Numbers, with an Application to\nthe Entscheidungsproblem.” Journal of Math\n58 (5): 345–63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A — A fun appendix",
    "section": "",
    "text": "The contents of this appendix are not here!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>A fun appendix</span>"
    ]
  },
  {
    "objectID": "intro.html#early-18th-century-the-birth-of-programming",
    "href": "intro.html#early-18th-century-the-birth-of-programming",
    "title": "1  The Lore",
    "section": "",
    "text": "1.1.1 Jacquard’s Loom: the punched card\nOne of the first programmable machines was Jacquard’s loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard’s loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard’s punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans. The meager statistics that existed were to be computed by hand.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830’s he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division.\nThe analytical engine stored numbers in the same format Babbage and Lovelace did: as decimals. A number like 17 would be split and stored as decimals: one ten and seven ones. Although Babbage fully designed and began to construct the analytical engine, it was never completed.\nDuring the 1830’s and 1840’s, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations: addition, subtraction, multiplication, and division. These were the only four operations the analytical engine was capable of carrying out.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, whose father was the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific music (Tibees 2020).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "intro.html#middle-and-late-18th-century",
    "href": "intro.html#middle-and-late-18th-century",
    "title": "1  The Lore",
    "section": "1.2 Middle and Late 18th century",
    "text": "1.2 Middle and Late 18th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 18th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution terrified white men of the institution that was already beginning to lose economic utility. Abolition became the law of most empires. Within the space of 40 years from 1830 until 1870, abolition was adopted in the British empire (except India, I think), the Russian empire (serfdom), the French empire, the Dutch empire/the Dutch East India Company, the Portuguese empire and the American empire.\nAlso within that 40 year period was:\n\nSamuel Colt’s invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836?)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman’s rights convention in the U.S. (Seneca Falls Convention, 1948)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling’s invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction of the Suez Canal (1860’s)\nMendel’s publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel’s invention of dynamite (1867)\nMarx’ publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev’s publication of the first periodic table (1869)\n\nBefore R, there was S (“S (Programming Language)” n.d.). S was developed by Bell Labs in the mid-1970’s. At the time Bell Labs was owned by the American Telephone and Telegraph (AT&T) monopoly. Initially, in the late 19th and early 20th century, it was believed that a monopoly was required to establish adequate telephone and telegraph services, particularly in rural areas which competitive forces encourage firms to overlook (Chang, Hen, and Kan n.d.). Alexander Graham Bell had invented and patented the telephone in 1876, leaving AT&T as the only firm having the intellectual property rights to produce telephones, which the government had decided would be a monopoly.\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T’s service, nor did it prevent future anti-trust lawsuits. Throughout the 60’s and early 70’s, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nS was developed by the statistical research departments at Bell Labs starting in 1976, two years after the beginning of the lawsuit that would break up AT&T and a full 20 years after a consent decree forbade AT&T (or, by association, Bell Labs) from selling computers.\n\n\nshow code for this result\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. “Case Study 1: AT&T Divestiture.” Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\n“S (Programming Language).” n.d. Wikipedia. Accessed May 6, 2024. https://en.wikipedia.org/w/index.php?title=S_(programming_language)&oldid=1212530456.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "intro.html#middle-and-late-19th-century",
    "href": "intro.html#middle-and-late-19th-century",
    "title": "1  The Lore",
    "section": "1.2 Middle and Late 19th century",
    "text": "1.2 Middle and Late 19th century\n\n1.2.1 1830-1870(ish)\nThe middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution terrified white men of the institution that was already beginning to lose economic utility. Abolition became the law of most empires. Within the space of 40 years from 1830 until 1870, abolition was adopted in the British empire (except India, I think), the Russian empire (serfdom), the French empire, the Dutch empire/the Dutch East India Company, the Portuguese empire and the American empire.\nAlso within that 40 year period was:\n\nSamuel Colt’s invention of a revolver that can be mass-produced (1836?)\nthe development of the telegraph (1830s)\nthe trail of tears (starting 1836?)\nthe revolutions of 1848 and the publication of the communist manifesto\nthe first woman’s rights convention in the U.S. (Seneca Falls Convention, 1848)\nthe discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)\nDarwin published On the Origin of Species (1859)\nGatling’s invention of the machine gun (1861)\nMaxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)\nthe construction and openning of the Suez Canal (1860’s)\nMendel’s publication of his laws of genetic inheritance (1865)\nthe discovery of the cell and subsequent elaboration of cell theory (1865 and after)\nNobel’s invention of dynamite (1867)\nMarx’ publication of the first volume of capital (1867)\nthe completion of the transcontinental railroad (U.S., 1869)\nMendeleev’s publication of the first periodic table (1869)\n\nIn this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.\n\n\n1.2.2 Late 19th century\nWith the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.\nWilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median (Bakker and Gravemeijer 2006). Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.\nAlso in the late 19th century was what Foucault called the implantation of perversions (Foucault 1978) - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.\nFirstly, there was the medical specification of the homosexual (Townsend 2011). This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female (Townsend 2011). The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.\nAlso within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.\nI’ll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of “good moral character” were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.\nUntil 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the “illegal alien” was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.\nAll this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.\nI consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I’m going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.\nThe late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.\nAnother monopoly was also forming. Using Jacquard’s punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "intro.html#att-bell-labs-and-s",
    "href": "intro.html#att-bell-labs-and-s",
    "title": "1  The Lore",
    "section": "1.4 AT&T, Bell Labs, and S",
    "text": "1.4 AT&T, Bell Labs, and S\nMonopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business (Chang, Hen, and Kan n.d.).\nThis consent decree did not prevent the further degradation of AT&T’s service, nor did it prevent future anti-trust lawsuits. Throughout the 60’s and early 70’s, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies (Chang, Hen, and Kan n.d.).\nBell Labs was probably the most important laboratory within AT&T. During the early 70’s, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).\nFORTRAN, the name, stands for “formula translation,” and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on its website.\nIn any case, in the 1970’s, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs (Becker 1994).\n\n1.4.1 S\n\n\n\n\n\n\nBakker, Arthur, and Koeno P. E. Gravemeijer. 2006. “An Historical Phenomenology of Mean and Median.” Educational Studies in Mathematics 62 (2): 149–68. https://www.jstor.org/stable/25472093.\n\n\nBecker, Richard A. 1994. “A Brief History of S.” In Computational Statistics, edited by Peter Dirschedl and Rüdiger Ostermann, 81–110. Heidelberg: Physica-Verlag HD. https://doi.org/10.1007/978-3-642-57991-2_6.\n\n\nChang, Grace, Elaine Hen, and Lili Kan. n.d. “Case Study 1: AT&T Divestiture.” Accessed May 6, 2024. https://inst.eecs.berkeley.edu/~eecsba1/sp97/reports/eecsba1e/final_proj/case1.html.\n\n\nFoucault, Michel. 1978. The History of Sexuality. Vol. 1. 3 vols. Random House.\n\n\nShustek, Leonard J. 2016. “Programming the ENIAC: An Example of Why Computer History Is Hard.” May 18, 2016. https://computerhistory.org/blog/programming-the-eniac-an-example-of-why-computer-history-is-hard/.\n\n\nTibees, dir. 2020. The First Computer Program. https://www.youtube.com/watch?v=_JVwyW4zxQ4.\n\n\nTownsend, Kristin. 2011. “The Medicalization of ‘Homosexuality’.” Honors Capstone Projects - All, May. https://surface.syr.edu/honors_capstone/292.\n\n\nTuring, Alan. 1936. “On Computable Numbers, with an Application to the Entscheidungsproblem.” Journal of Math 58 (5): 345–63. https://www.wolframscience.com/prizes/tm23/images/Turing.pdf.",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "intro.html#early-19th-century-the-advent-of-computing",
    "href": "intro.html#early-19th-century-the-advent-of-computing",
    "title": "1  The Lore",
    "section": "1.3 Early 19th century: the advent of computing",
    "text": "1.3 Early 19th century: the advent of computing\nNear the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:\n\nto be consistent: it should not be possible to derive that a statement is both true and false\nto be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)\nto be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.\n\n(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that \\(1+1=2\\). I’m mostly attributing these three demands to Hilbert for sanity’s sake because I cannot stand to write out the sordid details. These three “properties” as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of 23 questions, which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)\nMathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein’s theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.\nNeils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920’s quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The “demands of power” did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.\nDuring my quantum mechanical tangent, Gödel has proven that achieving the second property of Hilbert’s idealistic system is unlikely. In fact, Gödel establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.\nTo answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements (Turing 1936). There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).\nThus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term Turing-complete which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in (Turing 1936). The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard’s loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from (Turing 1936).\nThe first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine ENIAC. Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine (Shustek 2016). The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).\nInitially, the wires on ENIAC had to be moved for each new problem (Shustek 2016). The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.\nThis machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.\nThis is the idea behind modern programming languages. Instructions for the computer, written in the computer’s language (ENIAC’s language was wires, the one we’ll soon focus on is R) are stored within the machine. “Programming” the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC “program” ran under this new computer architecture was a Monte Carlo simulation of neutron decay during nuclear fission (Shustek 2016).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  },
  {
    "objectID": "intro.html#early-19th-century-the-birth-of-programming",
    "href": "intro.html#early-19th-century-the-birth-of-programming",
    "title": "1  The Lore",
    "section": "",
    "text": "1.1.1 Jacquard’s Loom: the punched card\nOne of the first programmable machines was Jacquard’s loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard’s loom was a machine that could create a variety of different patterns, depending on which program was put into it?\nSo, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard’s punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.\n\n\n1.1.2 Babbage, Lovelace, and the Analytical Engine\nGeneral-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans. The meager statistics that existed were to be computed by hand.\nOne of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830’s he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division.\nThe analytical engine stored numbers in the same format Babbage and Lovelace did: as decimals. A number like 17 would be split and stored as decimals: one ten and seven ones. Although Babbage fully designed and began to construct the analytical engine, it was never completed.\nDuring the 1830’s and 1840’s, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers (Tibees 2020). Her program, written in the iconic note G, used only the four simple arithmetic operations: addition, subtraction, multiplication, and division. These were the only four operations the analytical engine was capable of carrying out.\nLovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, whose father was the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific music (Tibees 2020).",
    "crumbs": [
      "R and Friends",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Lore</span>"
    ]
  }
]