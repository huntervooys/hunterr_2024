# The Lore

## Early 19th Century: the birth of programming

The idea of a programmable computer is not difficult to understand. The primary goal is to make a machine that you can use for different tasks, depending on what program you feed to that machine. Programs are written in code, which today is stored as text files on a computer. The programmable machine also lives within the computer, so running the program is as simple as telling the machine part of the computer where the program is located; and then the program runs.

Folks in the 19th century did not have access to digital text files, and so they could not write their computers on them. How did they write programs, and which sort of programs did they write? These are the primary questions I hope to address in this section?

### Jacquard's Loom: the punched card

One of the first programmable machines was Jacquard's loom, which Jacquard patented in 1804. Jacquard was a weaver. Jacquard's loom was a machine that could create a variety of different patterns, depending on which program was put into it?

So, Jacquard wrote programs to produce beautiful woven fabrics, but more important is how he wrote programs. Jacquard had no text files, so instead he developed a different form of machine input: the punched card. Each line on Jacquard's punched cards contained information about a single row of the design. The cards could be fed sequentially into the loom to produce a large pattern.

### Babbage, Lovelace, and the Analytical Engine

General-purpose digital computers, the sort of computers that can run R, emerged as an idea in the early-to-mid 19th century. Up to that point, computers were either mechanical (mechanical computers are fascinating, by the way) or just humans. The meager statistics that existed were to be computed by hand.

One of the first to develop a design for a general-purpose computer was Charles Babbage, working in the early part of the 19th century. In the 1830's he proposed a massively complicated, general-purpose, steam-powered computer, which he called the analytical engine. The computer was only capable of carrying out the four basic operations of arithmetic: addition, subtraction, multiplication, and division.

The analytical engine stored numbers in the same format Babbage and Lovelace did: as decimals. A number like 17 would be split and stored as decimals: one ten and seven ones. Although Babbage fully designed and began to construct the analytical engine, it was never completed.

During the 1830's and 1840's, Lady Ada Lovelace communicated with Charles Babbage (and several others involved in similar work) with the intention to collaborate with him in studying the analytical engine. It was Lady Lovelace who wrote the first substantial computer program, whose purpose was to compute Fibonacci numbers [@tibeesFirstComputerProgram2020]. Her program, written in the iconic note G, used only the four simple arithmetic operations: addition, subtraction, multiplication, and division. These were the only four operations the analytical engine was capable of carrying out.

Lovelace was interested in discovering the capabilities of the analytical engine. Her program computing Fibonacci numbers was important because it used loops in computation. Lovelace, whose father was the poet Lord Byron, was also interested in non-mathematical applications for the machine. She suggested that a sufficiently mathematical theory of sound could enable to engine to compose complex and scientific music [@tibeesFirstComputerProgram2020].

## Middle and Late 19th century

### 1830-1870(ish)

The middle of the 19th century was a period of massive global shifts. Liberation from enslavement was spreading across the globe after the Haitian revolution terrified white men of the institution that was already beginning to lose economic utility. Abolition became the law of most empires. Within the space of 40 years from 1830 until 1870, abolition was adopted in the British empire (except India, I think), the Russian empire (serfdom), the French empire, the Dutch empire/the Dutch East India Company, the Portuguese empire and the American empire.

Also within that 40 year period was:

-   Samuel Colt's invention of a revolver that can be mass-produced (1836?)

-   the development of the telegraph (1830s)

-   the trail of tears (starting 1836?)

-   the revolutions of 1848 and the publication of the communist manifesto

-   the first woman's rights convention in the U.S. (Seneca Falls Convention, 1848)

-   the discovery of the Bessemer Process which enables the mass-production of steel, paving the way for emerging steel tycoons (1855)

-   Darwin published On the Origin of Species (1859)

-   Gatling's invention of the machine gun (1861)

-   Maxwell publishes his equations, proposing an incredibly successful theory of physics that understands electricity, magnetism, and light as essentially the same thing (1861)

-   the construction and openning of the Suez Canal (1860's)

-   Mendel's publication of his laws of genetic inheritance (1865)

-   the discovery of the cell and subsequent elaboration of cell theory (1865 and after)

-   Nobel's invention of dynamite (1867)

-   Marx' publication of the first volume of capital (1867)

-   the completion of the transcontinental railroad (U.S., 1869)

-   Mendeleev's publication of the first periodic table (1869)

In this revisionist history of the computer (and ultimately of R), this period in history marked a transformation of power. The structure and organization of society was changing along with the flow of people, ideas, and commerce. Western, liberal democracies had to develop new technologies of population control in order to prevent all of these liberal changes from challenging their position of authority and power.

### Late 19th century

With the relative liberation of black bodies (and other bodies, as well) came a scientific imperative. Power continued to demand that these bodies be inferior, but evidence of inferiority was no longer to come from the conditions and dimensions of the body. Nay, the newly-available technologies of genetic inheritance and natural selection allowed a regime of a new flavor to take hold, one that cited hard science to support and justify the inequities in society. Inferiority was moving through the skin, into the body, and - importantly - into the mind.

Wilhelm Wundt opened the first psychology lab, and William James delivered the first psychology course and textbook. Galton, who was studying intelligence, popularized the idea of the median [@bakkerHistoricalPhenomenologyMean2006]. Psychology and with it psychological statistics, was beginning to take shape to meet the new demands of the state: a theory and a technology that will find permanent, internal traits upon which to stratify society into haves and have-nots. The story of the emergence of psychological statistics is incomplete without mention of eugenics. The tools being developed were not neutral and scientific, but overtly political, aimed at achieving the goals of the state.

Also in the late 19th century was what Foucault called the implantation of perversions [@foucaultHistorySexuality1978] - the creation of new symbolic threats to the body and to society as a whole. This operated through the invention of new characters that continue to exist within society today.

Firstly, there was the medical specification of the homosexual [@townsendMedicalizationHomosexuality2011]. This began in 1864 with the work of Karl-Heinrich Ulrichs, who was gay himself. He specified men as either urnings or dionings. Urnings and Dionings are both male-bodied creatures, but the urning experiences the desires and character of a female [@townsendMedicalizationHomosexuality2011]. The dioning, by contrast, is normal. Discourse about the urning (renamed to the invert, and then to the homosexual) continued well into the 20th century, and the sissy (the archetype the invert represents) is, obviously, still with us.

Also within this time period, was the medical specification of the hysteric woman, which was initially the perogative of Jean-Martin Charcot.

I'll mention just one more character that was invented in the later 19th century. For all of American history to this point, immigration law was about the process of naturalization - immigrants becoming citizens. From the beginning of the union, only white men of "good moral character" were allowed to become American citizens (Naturalization act of 1790?). There was little effort to actually prevent bodies from entering the country.

Until 1875. With the passage fo the Page Act of 1875, the United States declared its intention to keep undesirable bodies out of the country for the first time. Shortly thereafter, the "illegal alien" was invented as a result of the Chinese Exclusion Act of 1882, which is the only American immigration law I am aware of that names a specific national group in its title.

All this to say that the nature and enforcement of undesirability were in massive flux in the late 19th century. The foreign element was moving within: the enslaved African could become a citizen and could vote, the invert or the hysteric could be hiding within anyone, and the state took up the power to deport bodies that did not belong. No longer was the anthropologist writing about the inferiority of foreign peoples (although to be clear, they absolutely were still doing that); the pschiatrist was now writing about our own inferiority.

I consider the birth of statistics to be in this time period, which does not have pleasant implications for statistics as a field. There is a lot more to be said about the advent of statistics, and how statistics is designed to serve power (i.e., fulfill the demands of the state). However, I'm going to leave all of that unsaid and refocus on computation in general, and statistical computing in particular.

The late 19th century was also, notoriously, the era of massive trusts in the United States. These monstrous, monopolistic companies exploited both the consumer and the worker, but the United States did not yet have a legal mechanism for breaking them up. The most important monopoly for our purposes: the one that is most influential is the development of S and then R is the AT&T monopoly.

Another monopoly was also forming. Using Jacquard's punched cards, an American man designed and patented a system to read punched cards. In 1890, this punched card system was used to complete the census, resulting in the 1890 census being completed two years quicker than the 1880 one. The company that developed this technology would go on to become IBM, which enjoyed monopoly status in the computing industry for several decades.

## Early 19th century: the advent of computing

Near the end of the 19th century, a mathematician named David Hilbert decided that mathematics needed to be formalized. Up to that point, it had developed as myriad sub-disciplines that failed to cohere into a single, interconnected web of mathematics. Hilbert believed that it should, and his goal was to formalize this system. He believed that such a system (of mathematical axioms) needed to have three properties:

1.  to be consistent: it should not be possible to derive that a statement is both true and false
2.  to be complete: it should be possible to derive the truth of every true statement (or the falsity of its negation)
3.  to be decidable: there must be an algorithm that can identify all and only true statements in a finite number of steps.

(The excitement about formalizing affected Hilbert, but by no means was he the first or the only to be caught up in this mess. Notoriously, Whitehead and Russel got spun up enough to publish a 126-page long proof that $1+1=2$. I'm mostly attributing these three demands to Hilbert for sanity's sake because I cannot stand to write out the sordid details. These three "properties" as I call them, are really inspired very loosely on any specific, cite-able Hilbert publication. He did publish a list of [23 questions](https://en.wikipedia.org/wiki/Hilbert%27s_problems), which refer to the properties I mention here, but understand this as a drastically over-simplified view of the mathematical debates unfolding at the time.)

Mathematics was not the only field to be heating up. There was growing speculation in physics that matter may not be as continuous as was previously assumed. In 1900, Max Planck published the first quantum theory in physics, which was aimed at modelling thermal radiation. Shortly thereafter, Albert Einstein published another quantum theory, this time aimed at modeling the the photoelectric effect. Both of these models used quantum stuff (i.e., minimal, discrete units of energy, creating measurements of energy that are always a multiple of the quantum unit), but the authors did not actually believe the world was quantum. Famously, Einstein's theories of relativity both rely on space-time being continuous. They merely believed quantized math was the best way to explain non-quantum physical phenomena.

Neils Bohr went the whole way, creating his model of the atom, with distinct, orbital electron shells. In the 1920's quantum mechanics, as we know it today, came into existence. It did not make Einstein happy. Einstein wanted a deterministic world, where each cause has an specific, reliable effect. Quantum mechanics is not a deterministic theory of physics, but a probabilistic one. I take this diversion into the physical sciences not only to stress that this is a transition period within the physical sciences, but to temper my claim from the previous section. The "demands of power" did no less to supercharge the development of statistics and probability than did rapid changes in the way we understand and model the physical world.

During my quantum mechanical tangent, Gödel has proven that achieving the second property of Hilbert's idealistic system is unlikely. In fact, Gödel establishes that it is logically impossible that any formal mathematical system could be complete, as defined above.

To answer the question about whether mathematics is decidable, a new technology is needed. Before a mathematician can make formal claims about the capabilities or limitations of algorithms in general (as Hilbert demanded), she must first provide a rigorous definition of an algorithm. Two mathematicians took up this task, Alonzo Church who developed the lambda calculus, and Alan Turing who developed the Turing machine. Both men reached the same conclusion: mathematics cannot be decidable. It is logically impossible to make an algorithm (a Turing machine) that can identify all and only true statements [@turingComputableNumbersApplication1936]. There are, as it turns out, hard limits on the types of problems algorithms are able to solve (at least in a finite number of steps).

Thus, Turing half accidentally created the field of computer science while trying to answer a question about the foundations of mathematics. This is also an opportune time to introduce the term **Turing-complete** which refers to anything (model of computation, programming language, a book of instructions used by a human computer) that can simulate the a Turing machine. Any Turing-complete system is essentially equivalent to the original Turing machine described in [@turingComputableNumbersApplication1936]. The analytical engine is (theoretically, of course, it never got built) Turing-complete; Jacquard's loom, by contrast, is not. Modern programming languages are, for the most part, Turing complete, meaning that any function you write in a modern programming language could be performed on the OG Turing machine from [@turingComputableNumbersApplication1936].

The first electric, digital computer was not fully constructed until 1945. It was built by and for the U.S. military, who named the machine [ENIAC](https://en.wikipedia.org/wiki/ENIAC). Thus, the first computations done on an electric, digital computer were intended to speed up the process of human and earthly destruction. ENIAC was a bunch of coordinated units that ran according to the placement of wires on the machine [@shustekProgrammingENIACExample2016]. The machine took IBM punched cards as input (remember the punched card monopolist from the end of the 19th century?).

Initially, the wires on ENIAC had to be moved for each new problem [@shustekProgrammingENIACExample2016]. The process of re-configuring the machine for each new problem was tedious, but it was possible, and so ENIAC was Turing-complete. However, having to physically move wires prevented the machine from achieving the utility of a modern programmable computer.

This machine was very quickly modified in a way that dramatically changed its function. Instead of having to move wires, and then feed the machine (punched card) instructions based on the position of those wires, it would be much faster permanently code instructions (functions) into the machine. Then, the input of the machine could describe the sequence of functions. You could achieve looping by instructing ENIAC to perform a function repeatedly and conditional (if-statement) execution by instructing ENIAC to skip functions in the sequence.

This is the idea behind modern programming languages. Instructions for the computer, written in the computer's language (ENIAC's language was wires, the one we'll soon focus on is R) are stored within the machine. "Programming" the machine involves telling it which instructions to perform and in which order. In 1948, the first ENIAC "program" ran under this new computer architecture was a **Monte Carlo simulation** of neutron decay during nuclear fission [@shustekProgrammingENIACExample2016].

## AT&T, Bell Labs, and S

Monopolies suck, and AT&T did as well. Throughout the beginnning of the century, it gradually became clear that the benefits of a monopolist teleohpne provider were not going to materialize. In 1949, the U.S. Department of Justice sued AT&T for violating the anti-trust act, and the resulting 1956 consent decree prohibited AT&T from entering the computer business [@changCaseStudyDivestiture].

This consent decree did not prevent the further degradation of AT&T's service, nor did it prevent future anti-trust lawsuits. Throughout the 60's and early 70's, the U.S. government dogged AT&T with recurrent anti-trust lawsuits. In 1974, the Department of Justice began their final lawsuit against a monopoly AT&T. Over the course of the next decade, the government proved that AT&T was leveraging its monopoly power to predatory ends, annihilating potential competitors and pricing services far beyond the cost to provide them. This lawsuit ended in 1982 with the dissolution of AT&T into 7 Regional Bell Operating Companies [@changCaseStudyDivestiture].

Bell Labs was probably the most important laboratory within AT&T. During the early 70's, the researchers at the statistics research department within Bell Labs was using the programming language FORTRAN. FORTRAN is a general purpose, compiled programming language, developed by IBM (the punched card guys).

FORTRAN, the name, stands for "formula translation," and it was primarily used for scientific computing, like computing weather models or doing computational physics - things that have to do with numbers, essentially. It is still used in these fields to some extent, although it is less popular for scientific computing than other, more recent programming languages, like R. FORTRAN is, computationally speaking, incredibly efficient, mostly by natively supporting parallel computation. For this reason, FORTRAN is still used to benchmark supercomputers. You can learn more about FORTRAN on [its website](https://fortran-lang.org/).

In any case, in the 1970's, the statistics research department at Bell Labs found FORTRAN to be somewhat insufficient, and they set out to develop a new language that would more fully suit their needs [@beckerBriefHistory1994].

### S
